# -*- coding: utf-8 -*-
"""ReadabilityGradeLevels (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19uqQH9qzw8VReaku0HNGWSgl47IKyw__

# import Libraries
"""

!pip install syllables
!pip install indic-nlp-library
!pip install pyphen
!pip install -U textblob
!pip install pyspellchecker
!pip install --upgrade language_tool_python
import language_tool_python
import nltk
from nltk.corpus import words
from nltk.tokenize import sent_tokenize,word_tokenize
from nltk.corpus import cmudict
nltk.download('words')
nltk.download('cmudict')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from textblob import TextBlob
import math
import re
import string
import syllables
d = cmudict.dict()
tool = language_tool_python.LanguageTool('en-US')
from spellchecker import SpellChecker
spell = SpellChecker() 
! pip install langdetect
from langdetect import detect
import requests, time
url = 'https://farasa-api.qcri.org'
import ast

"""# All Functions"""

"""
#my source is: https://readabilityformulas.com/the-LIX-readability-formula.php
def w_g_4(txt):
    count =0
    words=word_tokenize(txt)
    for x in words:
        #print(len(x))
        if len(x) > 4:
            count+=1
    return count 
 
def LIX(txt):
  WordCount=wordCount(txt)
  SentenceCount=sentenceCount(txt)
  LongWords=w_g_4(txt)
  percentageOfLongWords=(LongWords/WordCount)*100
  avgLengthOfSentence=WordCount/SentenceCount
  result=round(percentageOfLongWords+avgLengthOfSentence,0)
  return result
 
 
def LensearWrite(text):
  text=removePunctuation(text)
  words=word_tokenize(text)
  SentenceCount=sentenceCount(text)
  wordcount=wordCount(text)
  #hardword=3
  #easyword=1
  ratio=100/wordcount
  hardword=ratio*3
  easyword=ratio*1
  Score=0
  for word in words:
    if nsyl(word) <= 2:
      Score+=easyword
    if nsyl(word) >=3:
      Score+=hardword
 
  #preResult=((Score/SentenceCount)/wordcount)*100
  preResult=(Score/SentenceCount)
  print(preResult)
  #print(wordCount(text))
  if preResult > 20:
    result=round((preResult/2),1)
  else:
    result=round((preResult-2)/2,1)
    
  return result
 
  
print(LIX(t))
print(LensearWrite(t))
 
 
"""
 
 
 
 
def removePunctuation(text):
    result = ""
 
    for char in text:
        if char in (".",",","!","?","؟","،","\",""/","\"","#","$","%","&","'","(",")","*","+",":",";","<",">","=","[","]","^","_","`","{","}","|","~"):
            continue
        if char in ("-","\n","\r","\t"):
            char=" "
        result+=char
    return result
 
def removeSpace(text):
    result=""
    for char in text:
        if char in ("-","\n","\r","\t"):
            char=" "
        result+=char
    return result
 
def removeDigits(text):
    result=""
    for i in text:
        if(i.isdigit()):
            continue
        result+=i
    return result
 
def characterCount(text):
    count=0
    text=removePunctuation(text)
    text = re.sub('[ًٌٍَُِّْـ]+', '', text)
    for char in text:
        if (char.isdigit() or char.isspace()):
            continue;
        count+=1
    #print("Character Count is ",count)
    return count
 
def wordCount(text):
    count=0
    text=removePunctuation(text)
    words=word_tokenize(text)
    #print(words)
    for i in words:
        if(i.isdigit()):
            continue;
        count+=1
    #print("Word Count is ",count)
    return count
   
def count_complex_word(text):
    complex_words = 0
    for i in text.split(' '):        
        syllable_count = syllables(i)
        if (syllable_count >= 3):
            complex_words +=1
    #print("Complex Word Count is ",complex_words)
    return max(1, complex_words)
 
def nsyl(word):
    try:
        #print(d[word.lower()])
        li=[]
        #print(word,"-",d[word.lower()])
        for x in d[word.lower()][0]:
            for y in x:
                #print(y)
                if(y[-1].isdigit()):
                    li.append(y)
        #print(word,":",len(li))
        return(len(li))
    except:
        #if word not found in cmudict
        return(syllables(word))
 
def syllables(word):
    count = 0
    vowels = 'aeiouy'
    word = word.lower()
    if word[0] in vowels:
        count +=1
    for index in range(1,len(word)):
        if word[index] in vowels:
            if word[index-1] not in vowels:
                count +=1
    if word.endswith('e'):
        count -= 1
    if word.endswith('le'):
        count += 1
    if count == 0:
        count += 1
    #print("Count:",count)
    return count
 
def syllableCount(text):
    count=0
    num=0
    text=removePunctuation(text)
    words=word_tokenize(text)
    #print(words)
    for i in words:
        count=count+nsyl(i)
    return(count)
       
def singleSyllableWord(text):
    text=removePunctuation(text)
    text=removeDigits(text)
    words=word_tokenize(text)
    result=0
    for i in words:
        count=0
        count=nsyl(i)
        if(count==1):
            #print(i)
            result=result+1
    #print(result)
    #print("Single Syllable Words are: ",result)
    return result
 
def uniqueWordCount(text):
    count=0
    text=removePunctuation(text)
    text=text.lower()
    words=word_tokenize(text)
    uniqueWords=[]
    for i in words:
        if i not in uniqueWords:
            #print(i)
            uniqueWords.append(i)
    for word in uniqueWords:
        if word.isdigit():
            continue;
        count+=1
    #print("Unique Count is ",count)
    return count
   
"""def sentenceCount(text):
    count = 0 
    for i in text:
      if i in(".","?","!","؟"):
        count+=1
    return max(1,count)"""

def sentenceCount(text):
  count = 0
  text2 = text + "\n"
  text2 = text2.replace(".","\n")
  text2 = text2.replace("!","\n")
  text2 = text2.replace("?","\n")
  text2 = text2.replace("؟","\n")
  lines = text2.split("\n")
  for line in lines:
    line = line.strip()
    if len(line) == 0:
      continue
    count+=1   
    if count == 0:
      count = 1 
  return count     
def paragraphCount(text):
    count=  [line.strip() for line in text.split('\n') if len(line.strip()) > 0]
    #print("Paragraph Count is",len(count))
    return len(count)
 
def ReadingTime(text):
    word = wordCount(text)
    div_wordCount = word / 225
    second,minute=math.modf(div_wordCount)
    if second >= 0.60:
      minute +=1
      second-=0.60
    #second=round((second*0.60),2)
    #print("Reading Time is",minute+second);
    return int(minute),int(round(second,2)*100)
 
def SpeakingTime(text):
    word = wordCount(text)
    speakingRate=125
    time=word/speakingRate
    second,minute=math.modf(time)
    if second >= 0.60:
      minute +=1
      second-=0.60
    #second=round((second*0.60),2)
    #print("Reading Time is",minute+second);
    return int(minute),int(round(second,2)*100)
 
def FKRA(txt):
    #txt = txt.lower()
    #txt=removePunctuation(txt)
    word = wordCount(txt)
    sentence =sentenceCount(txt)
    syllable =syllableCount(txt) 
    ASL = float(word)/sentence
    ASW = float(syllable)/word
    FKRA = (0.39 * ASL) + (11.8 * ASW) - 15.59
    return round(FKRA,2)
 
def GFI(txt):
    #txt=removePunctuation(txt)
    word = wordCount(txt)
    sentence = sentenceCount(txt)
    #syllable =syllableCount(txt)
    c_word = count_complex_word(txt)
 
    GFI = 0.4 * ((float(word) / sentence) + 100 * (float(c_word) / word))
    return round(GFI,2)
 
def CLI(txt):
    txt = txt.lower()
    char = characterCount(txt)
    word = wordCount(txt)
    sentence = sentenceCount(txt)
    L = (float(char) / word) * 100
    S = (float(sentence) / word) * 100
    CLI = (0.0588 * L) - (0.296 * S) - 15.8
    return round(CLI,2)
 
def SMOGI(txt):
    txt = txt.lower()
    sentence = sentenceCount(txt)
    plosyllable = count_complex_word(txt)
    SMOGI = 1.0430 * (math.sqrt(plosyllable * (30 / float(sentence)))) + 3.1291
    return round(SMOGI,2)
 
def ARI(txt):
    txt = txt.lower()
    char =characterCount(txt)
    word = wordCount(txt)
    sentence =sentenceCount(txt)
    ARI = (4.71 * (float(char) / word)) + (.5 * (float(word) / sentence)- 21.43)
    return round(ARI , 2)
 
def FORCAST(txt):
    syll=singleSyllableWord(txt)
    c_word = count_complex_word(txt)
    div=round((float(wordCount(txt))*10)/150,2)
    #print(div)
    GL = 20 - (float(syll)/ div)
    return round(GL,2)
 
def PSKG(txt):
    #txt=removePunctuation(txt)
    word = wordCount(txt)
    sentence = sentenceCount(txt)
    syllable = syllableCount(txt)
    ASL = float(word) / sentence
    #NS = (syllable * 0.0455) 
    #GL = (0.0778 * ASL) + NS + 2.7971
    NS = ((float(syllable)/word) * 0.0455*100) 
    GL = (0.0778 * ASL) + NS - 2.2029
    return round(GL,2)
 
 
def RIX(txt):
    count = 0
    sentence = sentenceCount(txt)
    for x in txt.split(" "):
        #print(characterCount(x))
        if characterCount(x) > 4:
            count+=1
    return round(float(count) / sentence,2)   
 
def FRE(txt):
    sentence = sentenceCount(txt)
    word = wordCount(txt)
    syllable = syllableCount(txt)
    #print(word)
    #print(sentence)
    #print(syllable)
    ASL = float(word) / sentence
    ASW = float(syllable) / word
    RE = 206.835 - (1.015 * ASL) - (84.6 * ASW)
    return round(RE,1)
 
def NDC(txt):
    c_word = count_complex_word(txt)
    word = wordCount(txt)
    sentence = sentenceCount(txt)
    PDW = round(float(c_word) / word,2)*100
    ASL = float(word) / sentence
    if PDW > 5:
        RS = ((0.1579 * PDW) + (0.0496 * ASL)) + 3.6365
    else:
        RS = (0.1579 * PDW) + (0.0496 * ASL)
    return round(RS,1)
 
def SPACHE(txt):
    c_word = count_complex_word(txt)
    word = wordCount(txt)
    sentence = sentenceCount(txt)
    PDW = round(float(c_word) / word,2)*100
    ASL = float(word) / sentence
    SP = ((0.141 *ASL) + (0.086 * PDW)+0.839)
    return round(SP,1)  
 
#Sentences > 30 Syllables
def s_g_30s(txt):
    count =0
    sentence=sent_tokenize(txt)
    for x in sentence:
        #print(syllableCount(x))
        if syllableCount(x) > 30:
            count+=1
    return count    
 
#Sentences > 20 Syllables
def s_g_20s(txt):
    count =0
    sentence=sent_tokenize(txt)
    for x in sentence:
        #print(syllableCount(x))
        if syllableCount(x) > 20:
            count+=1
    return count    
 
#Words > 4 Syllables
def w_g_4s(txt):
    count =0
    words=word_tokenize(txt)
    for x in words:
        #print(syllableCount(x))
        if syllableCount(x) >= 4:
            count+=1
    return count  
 
#Words > 12 Letters
def w_g_12l(txt):
    count =0
    words=word_tokenize(txt)
    for x in words:
        #print(len(x))
        if len(x) > 12:
            count+=1
    return count 
 
# Characters Per Word
def CPW(txt):
  WordCount=wordCount(txt)
  CharacterCount=characterCount(txt)
  CharactersPerWord=round(float(CharacterCount)/WordCount,1)
  return CharactersPerWord
 
#Syllables Per Word
def SPW(txt):
  WordCount=wordCount(txt)
  SyllableCount= syllableCount(txt)
  SyllablesPerWord=round(float(SyllableCount)/WordCount,1)
  return SyllablesPerWord
 
#Words Per Sentence
def WPS(txt):
  WordCount=wordCount(txt)
  SentenceCount=sentenceCount(txt)
  WordsPerSentence=round(float(WordCount)/SentenceCount,1)
  return WordsPerSentence
 
#Words Per Paragraph
def WPP(txt):
  WordCount=wordCount(txt)
  ParagraphCount=paragraphCount(txt)
  WordsPerParagraph=round(float(WordCount)/ParagraphCount,1)
  return WordsPerParagraph
 
#Sentences Per Paragraph
def SPP(txt):
  SentenceCount=sentenceCount(txt)
  ParagraphCount=paragraphCount(txt)
  SentencePerParagraph=round(float(SentenceCount)/ParagraphCount,1)
  return SentencePerParagraph
 
def w_g_6l(txt):
    count =0
    words=word_tokenize(txt)
    for x in words:
        #print(len(x))
        if len(x) > 6:
            count+=1
    return count 
 
def LIX(txt):
  WordCount=wordCount(txt)
  SentenceCount=sentenceCount(txt)
  LongWords=w_g_6l(txt)
  percentageOfLongWords=(float(LongWords)/WordCount)*100
  avgLengthOfSentence=float(WordCount)/SentenceCount
  result=round(percentageOfLongWords+avgLengthOfSentence,0)
  return result
 
def process_content(text):
  
    dict_group={"Adjectives":["JJ","JJR","JJS"],"Adverbs":["RB","RBR","RBS"],"Conjuctions":["CC"],"Determiners":["DT","PDT","WDT"],"Interjections":["UH"],"Nouns":["NN","NNS"],"Proper Nouns":["NNP","NNPS"],"Prepositions":["IN"],"Pronouns":["PRP","PRP$","WP","WP$"],"Verbs":["VB","VBD","VBG","VBN","VBP","VBZ"],}
    dict_count={"Adjectives":0,"Adverbs":0,"Conjuctions":0,"Determiners":0,"Interjections":0,"Nouns":0,"Proper Nouns":0,"Prepositions":0,"Pronouns":0,"Verbs":0}
    #dict_group={"Adjectives":["JJ","JJR","JJS"],"Adverbs":["RB","RBR","RBS"],"Conjuctions":["CC"],"Determiners":["PDT","WDT"],"Interjections":["UH"],"Nouns":["NN","NNS"],"Proper Nouns":["NNP","NNPS"],"Prepositions":["IN"],"Pronouns":["PRP","PRP$","WP","WP$"],"Verbs":["VB","VBD","VBG","VBN","VBP","VBZ"],}
    #dict_count={"Adjectives":0,"Adverbs":0,"Conjuctions":0,"Determiners":0,"Interjections":0,"Nouns":0,"Proper Nouns":0,"Prepositions":0,"Pronouns":0,"Verbs":0}
    tokenized=sent_tokenize(text)
    try:
        for tokens in tokenized:
            words = nltk.word_tokenize(tokens)
            tagged = nltk.pos_tag(words)
            for word in tagged:
                pos=word[1]
                for listKey,listElem in dict_group.items():    
                    if pos in listElem:
                        dict_count[listKey]+=1
    except Exception as e:
        print(str(e))
    #Final Result Printed.
    #print(dict_count)
    TextComposition=list(dict_count.items())
    return(TextComposition)
  
def getAdjectivesCount():
  return TextComposition[0][1]
 
def getAdverbsCount():
  return TextComposition[1][1]
 
def getConjuctionsCount():
  return TextComposition[2][1]
 
def getDeterminersCount():
  print("Answer-",TextComposition[3][1])
  return TextComposition[3][1]
 
def getInterjectionsCount():
  return TextComposition[4][1]
 
def getNounsCount():
  return TextComposition[5][1]
 
def getPropernounCount():
  return TextComposition[6][1]
 
def getPrepositionsCount():
  return TextComposition[7][1]
 
def getPronounsCount():
  return TextComposition[8][1]
 
def getVerbsCount():
  return TextComposition[9][1]
 
def nonWordCount(text):
    regex="[a-z0-9]+[\._]?[a-z0-9]+[@]\w+[.]\w+$"    #checks for email address
    #text=removePunctuation(text)
    emails = re.findall("([a-zA-Z0-9]+[\._]?@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+)", text)
    print("Non word: ",emails)
    return(len(emails))
 
def LensearWrite(text):
  text=removePunctuation(text)
  words=word_tokenize(text)
  SentenceCount=sentenceCount(text)
  wordcount=wordCount(text)
  #hardword=3
  #easyword=1
  ratio=100/float(wordcount)
  hardword=ratio*3
  easyword=ratio*1
  Score=0
  for word in words:
    if nsyl(word) <= 2:
      Score+=easyword
    if nsyl(word) >=3:
      Score+=hardword
  preResult=(Score/float(SentenceCount))
  if preResult > 20:
    result=round((preResult/2),1)
  else:
    result=round((preResult-2)/2,1)
    
  return result
 
def take_data_to_shower(text):
    text=removeSpace(text)
    alphanumeric = ""
    for char in text:
        if char ==" ":
            alphanumeric += " "
            continue
        if char.isalnum():
            alphanumeric += char 
    #print(alphanumeric)
    return alphanumeric
 
def spellingIssues(t):
    text = take_data_to_shower(t)
    wrong =[]
    for w in (text.lower()).split():
        flag = str(w)in words.words()
        if flag == False and w not in wrong:
            wrong.append(w)
    return len(wrong)
tool = language_tool_python.LanguageTool('en-US')
 
def grammarIssues(t):
  matches = tool.check(t)
  return (len(matches))
 
def isPassive(sentence):
    beforms = ['am', 'is', 'are', 'been', 'was', 'were', 'be', 'being']              
    aux = ['do', 'did', 'does', 'have', 'has', 'had']                                 
    words = nltk.word_tokenize(sentence)
    tokens = nltk.pos_tag(words)
    tags = [i[1] for i in tokens]
    if tags.count('VBN') == 0:                                                           
        return False
    elif tags.count('VBN') == 1 and 'been' in words:                                   
        return False
    else:
        pos = [i for i in range(len(tags)) if tags[i] == 'VBN' and words[i] != 'been']  
        end=pos[0]
        chunk = tags[:end]
        start = 0
        for i in range(len(chunk), 0, -1):
            last = chunk.pop()
            if last == 'NN' or last == 'PRP':
                start = i                                                             
                break
        sentchunk = words[start:end]
        tagschunk = tags[start:end]
        verbspos = [i for i in range(len(tagschunk)) if tagschunk[i].startswith('V')] 
        if verbspos != []:                                                           
            for i in verbspos:
                if sentchunk[i].lower() not in beforms and sentchunk[i].lower() not in aux:  
                    break
            else:
                return True
    return False
 
def passiveCount(text):
    sentences= nltk.sent_tokenize(text)
    count=0
    for sent in sentences:
        val=isPassive(sent)
        if(val==True):
            count+=1
    return count
 
 
def en_results(t):
 
  print("############################################################Text Statistics############################################################")
  print("Character Count : {}".format(characterCount(t)))
  print("Syllable Count : {}".format(syllableCount(t)))
  print("Word Count : {}".format(wordCount(t)))
  print("Unique Word Count : {}".format(uniqueWordCount(t)))
  print("Sentence Count : {}".format(sentenceCount(t)))
  print("Paragraph Count : {}".format(paragraphCount(t)))
  print("Reading Time: {}".format(ReadingTime(t)))
  print("Speaking Time: {}".format(SpeakingTime(t)))
  print("############################################################Text Composition############################################################")
  print("Adjectives : {}".format(getAdjectivesCount()))
  print("Adverbs : {}".format(getAdverbsCount()))
  print("Conjuctions : {}".format(getConjuctionsCount()))
  print("Determiners : {}".format(getDeterminersCount()))
  print("Interjections : {}".format(getInterjectionsCount()))
  print("Nouns : {}".format(getNounsCount()))
  print("Proper Nouns : {}".format(getPropernounCount()))
  print("Prepositions : {}".format(getPrepositionsCount()))
  print("Pronouns : {}".format(getPronounsCount()))
  print("Verbs : {}".format(getVerbsCount()))
  print("############################################################Readability Grade Levels############################################################")
  print("Flesch-Kincaid Grade Level : {}".format(FKRA(t)))
  print("Gunning Fog Index : {}".format(GFI(t)))
  print("Coleman–Liau index : {}".format(CLI(t)))
  print("SMOG Index : {}".format(SMOGI(t)))
  print("Automated Readability Index : {}".format(ARI(t)))
  print("FORCAST Grade Level : {}".format(FORCAST(t)))
  print("Powers Sumner Kearl Grade : {}".format(PSKG(t)))
  print("RIX Readability : {}".format(RIX(t)))
  print("############################################################Readability Scores############################################################")
  print("Flesch Reading Ease : {}".format(FRE(t)))
  print("Spache Score : {}".format(SPACHE(t)))
  print("New Dale-Chall Score : {}".format(NDC(t)))
  print("Lix Readability: {}".format(LIX(t)))
  print("Lensear Write: {}".format(LensearWrite(t)))
  print("############################################################Readability Issues############################################################")
  print("Sentences > 30 Syllables = {}\t{}%".format(s_g_30s(t),int((s_g_30s(t)/sentenceCount(t)*100))))
  print("Sentences > 20 Syllables = {}\t{}%".format(s_g_20s(t),int((s_g_20s(t)/sentenceCount(t)*100))))
  print("Words > 4 Syllables= {}\t{}%".format(w_g_4s(t),int((w_g_4s(t)/wordCount(t)*100))))
  print("Words > 12 Letters= {}\t{}%".format(w_g_12l(t),int((w_g_12l(t)/wordCount(t)*100))))
  print("############################################################Text Density Issues############################################################")
  print("Characters Per Word : {}".format(CPW(t)))
  print("Syllables Per Word : {}".format(SPW(t)))
  print("Words Per Sentence : {}".format(WPS(t)))
  print("Words Per Paragraph : {}".format(WPP(t)))
  print("Sentences Per Paragraph : {}".format(SPP(t)))
  print("Grammar Issues : {}".format(grammarIssues(t)))
  print("Spelling Issues: {}".format(spellingIssues(t)))
  print("Passive Voice Count : {}".format(passiveCount(t))) 
 
#############################################################################################################################################
                                                               #Arabic 
##############################################################################################################################################
 
def Seg(text):
  apiURL = "/msa/webapi/segmenter"
  text = {'text': text}
  headers = { 'content-type': "application/json", 'cache-control': "no-cache" }
  response = requests.get(url + apiURL, params=text, headers=headers)
  result =  response.text
  res = ast.literal_eval(result)
  return res["segtext"]
 
 
 
def lemma(text):
  apiURL = "/msa/webapi/lemma"
  text = {'text': text}
  headers = { 'content-type': "application/json", 'cache-control': "no-cache" }
  response = requests.get(url + apiURL, params=text, headers=headers)
  result =  response.text
  res = ast.literal_eval(result)
  return res["result"]
  
 
def POS(text):
  apiURL = "/msa/webapi/pos"
  text = {'text': text}
  headers = { 'content-type': "application/json", 'cache-control': "no-cache" }
  response = requests.get(url + apiURL, params=text, headers=headers)
  result = response.text
  res = ast.literal_eval(result)
  arr=[] 
  for x in range(1,len(res)-1):
      r=res[x]
      arr.append(r["POS"])
 
  my_dict = {i:arr.count(i) for i in arr}
  for keys,values in my_dict.items():
      print(keys," :",values)
 
def SpCH(text):
  apiURL = "/msa/webapi/spellcheck"
  text = {'text': text}
  headers = { 'content-type': "application/json", 'cache-control': "no-cache" }
  response = requests.get(url + apiURL, params=text, headers=headers)
  result =  response.text
  res = ast.literal_eval(result)
  return res["result"]
 
def Diac(text):
  apiURL = "/msa/webapi/diacritize"
  text = {'text': text}
  headers = { 'content-type': "application/json", 'cache-control': "no-cache" }
  response = requests.get(url + apiURL, params=text, headers=headers)
  result =  response.text
  res = ast.literal_eval(result)
  return res["output"]
 
def Diac2(text):
  apiURL = "/msa/webapi/diacritizeV2"
  text = {'text': text}
  headers = { 'content-type': "application/json", 'cache-control': "no-cache" }
  response = requests.get(url + apiURL, params=text, headers=headers)
  result =  response.text
  res = ast.literal_eval(result)
  return res["output"]
 
def ar_syllables(word):
    #fatha, damma, kasra
    tashkeel = ['\u064E','\u064F','\u0650']
 
    count_long = 0
    count_short = 0
    count_stress = 0
 
    for x in range(0,len(tashkeel)):
        for i in range(0, len(word)):
            if word[i] == tashkeel[x]:
                if i+1 < len(word):
                    #to count long syllables we need to check if the character following is an alef, waw or yaaA.
                    if word[i+1]=='\u0627' or word[i+1]=='\u0648' or word[i+1]=='\u064a':
                        count_long += 1
                    else:
                        count_short += 1
                else:
                    count_short += 1
 
    #counts stress syllables, those tanween fatih, tanween damm, tanween kasr and shadda.
    count_stress = word.count("\u064B") + word.count("\u064C") + word.count("\u064D") + word.count("\u0651")
    #syllables_count = count_short + (2 * count_long) + (2 * count_stress)
    syllables_count = count_short +  count_long +  count_stress
    return syllables_count
 
def ar_syllables_count(text):
  words = text.split(" ")
  count = 0
  for i in words:
    count = count+ar_syllables(i)
  return count  
 
def ar_singleSyllableWord(text):
    words=word_tokenize(text)
    result=0
    for i in words:
        count=0
        count=ar_syllables(i)
        if(count==1):
            result=result+1
    return result
 
def ar_count_complex_word(text):
    complex_words = 0
    for i in text.split(' '):        
        syllable_count = ar_syllables(i)
        if (syllable_count >= 4):
            complex_words +=1
    #print("Complex Word Count is ",complex_words)
    return max(1, complex_words)
 
def ar_FKRA(txt):
    word = wordCount(txt)
    sentence =sentenceCount(txt)
    syllable =ar_syllables_count(txt) 
    ASL = float(word)/sentence
    ASW = float(syllable)/word
    FKRA = (0.39 * ASL) + (11.8 * ASW) - 15.59
    return round(FKRA,2)
 
def ar_GFI(txt):
    word = wordCount(txt)
    sentence = sentenceCount(txt)
    c_word = ar_count_complex_word(txt)
 
    GFI = 0.4 * ((float(word) / sentence) + 100 * (float(c_word) / word))
    return round(GFI,2)
 
def ar_CLI(txt):
    char = characterCount(txt)
    word = wordCount(txt)
    sentence = sentenceCount(txt)
    L = (float(char) / word) * 100
    S = (float(sentence) / word) * 100
    CLI = (0.0588 * L) - (0.296 * S) - 15.8
    return round(CLI,2)
 
def ar_SMOGI(txt):
    sentence = sentenceCount(txt)
    plosyllable = ar_count_complex_word(txt)
    SMOGI = 1.0430 * (math.sqrt(plosyllable * (30 / float(sentence)))) + 3.1291
    return round(SMOGI,2)
 
def ar_ARI(txt):
    char =characterCount(txt)
    word = wordCount(txt)
    sentence =sentenceCount(txt)
    ARI = (4.71 * (float(char) / word)) + (.5 * (float(word) / sentence)- 21.43)
    return round(ARI , 2)
 
def ar_FORCAST(txt):
    syll=ar_singleSyllableWord(txt)
    c_word = ar_count_complex_word(txt)
    div=round( (wordCount(txt)*10)/150,2)
    #print(div)
    GL = 20 - (float(syll)/ div)
    return round(GL,2)
 
def ar_PSKG(txt):
    word = wordCount(txt)
    sentence = sentenceCount(txt)
    syllable = ar_syllables_count(txt)
    ASL = float(word) / sentence
    NS = ((float(syllable)/word) * 0.0455*100) 
    GL = (0.0778 * ASL) + NS - 2.2029
    return round(GL,2)
 
 
def ar_RIX(txt):
    count = 0
    sentence = sentenceCount(txt)
    for x in txt.split(" "):
        if characterCount(x) > 4:
            count+=1
    return round(float(count) / sentence,2)  
 
def ar_FRE(txt):
    sentence = sentenceCount(txt)
    word = wordCount(txt)
    syllable = ar_syllables_count(txt)
    ASL = float(word) / sentence
    ASW = float(syllable) / word
    RE = 206.835 - (1.015 * ASL) - (84.6 * ASW)
    return round(RE,1)
 
def ar_NDC(txt):
    c_word = ar_count_complex_word(txt)
    word = wordCount(txt)
    sentence = sentenceCount(txt)
    PDW = round(float(c_word) / word,2)*100
    ASL = float(word) / sentence
    if PDW > 5:
        RS = ((0.1579 * PDW) + (0.0496 * ASL)) + 3.6365
    else:
        RS = (0.1579 * PDW) + (0.0496 * ASL)
    return round(RS,1)
 
def ar_SPACHE(txt):
    c_word = ar_count_complex_word(txt)
    word = wordCount(txt)
    sentence = sentenceCount(txt)
    PDW = round(float(c_word) / word,2)*100
    ASL = float(word) / sentence
    SP = ((0.141 *ASL) + (0.086 * PDW)+0.839)
    return round(SP,1)  
 
#Sentences > 30 Syllables
def ar_s_g_30s(txt):
    count =0
    sentence=sent_tokenize(txt)
    for x in sentence:
        #print(syllableCount(x))
        if ar_syllables_count(x) > 30:
            count+=1
    return count    
 
#Sentences > 20 Syllables
def ar_s_g_20s(txt):
    count =0
    sentence=sent_tokenize(txt)
    for x in sentence:
        #print(syllableCount(x))
        if ar_syllables_count(x) > 20:
            count+=1
    return count    
 
#Words > 4 Syllables
def ar_w_g_4s(txt):
    count =0
    words=word_tokenize(txt)
    for x in words:
        #print(syllableCount(x))
        if ar_syllables(x) >= 4:
            count+=1
    return count  
 
 

 
#Syllables Per Word
def ar_SPW(txt):
  WordCount=wordCount(txt)
  SyllableCount= ar_syllables_count(txt)
  SyllablesPerWord=round(float(SyllableCount)/WordCount,1)
  return SyllablesPerWord
 
def ar_spellingIssues(text):
   res = SpCH(text)
   count = 0
   for i in res:
     if i == '/':
       count +=1
   return count    
 
def Al_Heeti(text):
   words = wordCount(text)
   chars = characterCount(text)
   F3 = float(chars) / words
   Al = (F3 * 4.414 ) - 13.468
   return round(Al,2)

def ARI(text):
  words = wordCount(text)
  chars = characterCount(text)
  sentences = sentenceCount(text)
  F3 = float(chars) / words
  F5 = float(words) / sentences
  ARI = (F3*4.71) + (F5 * 0.5) - 21.43
  return round(ARI,2) 

def AARI(text):
  words = wordCount(text)
  chars = characterCount(text)
  sentences = sentenceCount(text)
  F3 = float(chars) / words
  F5 = float(words) / sentences
  AARI = ((chars*3.28)+(F3 * 1.43)+(F5 * 1.24)+472.42)/1046.3
  return round(AARI,2)

def ar_ReadingTime(text):
    word = wordCount(text)
    div_wordCount=float(word)/110
    second,minute=math.modf(div_wordCount)
    if second >= 0.60:
      minute +=1
      second-=0.60
    #second=round((second*0.60),2)
    #print("Reading Time is",minute+second);
    return int(minute),int(round(second,2)*100)

def ar_SpeakingTime(text):
    word = wordCount(text)
    speakingRate=84
    time=word/speakingRate
    second,minute=math.modf(time)
    if second >= 0.60:
      minute +=1
      second-=0.60
    #second=round((second*0.60),2)
    #print("Reading Time is",minute+second);
    return int(minute),int(round(second,2)*100)

def ar_results(t):
  print("############################################################Text Statistics############################################################")
  print("Character Count : {}".format(characterCount(t)))
  print("Syllable Count : {}".format(ar_syllables_count(t)))
  print("Word Count : {}".format(wordCount(t)))
  print("Unique Word Count : {}".format(uniqueWordCount(t)))
  print("Sentence Count : {}".format(sentenceCount(t)))
  print("Paragraph Count : {}".format(paragraphCount(t)))
  print("Reading Time: {}".format(ar_ReadingTime(t)))
  print("Speaking Time: {}".format(ar_SpeakingTime(t)))
  print("############################################################Farasa APIs############################################################")
  print("Segmentaion: {}".format(Seg(t)))
  print("Lemmatization: {}".format(lemma(t)))
  print("POS: {}".format(POS(t)))
  print("SpellChecker: {}".format(SpCH(t)))
  print("Diacritization: {}".format(Diac(t)))
  print("Diacritizationv2: {}".format(Diac2(t)))
  print("############################################################Arabic Measures############################################################")
  print("Al-Heeti Formula: {}".format(Al_Heeti(t)))
  print("ARI Formula: {}".format(ARI(t)))
  print("AARI Formula: {}".format(AARI(t)))
  print("############################################################Readability Grade Levels############################################################")
  print("Flesch-Kincaid Grade Level : {}".format(ar_FKRA(t)))
  print("Gunning Fog Index : {}".format(ar_GFI(t)))
  print("Coleman–Liau index : {}".format(ar_CLI(t)))
  print("SMOG Index : {}".format(ar_SMOGI(t)))
  print("Automated Readability Index : {}".format(ar_ARI(t)))
  print("FORCAST Grade Level : {}".format(ar_FORCAST(t)))
  print("Powers Sumner Kearl Grade : {}".format(ar_PSKG(t)))
  print("RIX Readability : {}".format(ar_RIX(t)))
  print("############################################################Readability Scores############################################################")
  print("Flesch Reading Ease : {}".format(ar_FRE(t)))
  print("Spache Score : {}".format(ar_SPACHE(t)))
  print("New Dale-Chall Score : {}".format(ar_NDC(t)))
  print("Lix Readability: {}".format(LIX(t)))
  print("Lensear Write: {}".format(LensearWrite(t)))
  print("############################################################Readability Issues############################################################")
  print("Sentences > 30 Syllables = {}\t{}%".format(ar_s_g_30s(t),int((ar_s_g_30s(t)/sentenceCount(t)*100))))
  print("Sentences > 20 Syllables = {}\t{}%".format(ar_s_g_20s(t),int((ar_s_g_20s(t)/sentenceCount(t)*100))))
  print("Words > 4 Syllables= {}\t{}%".format(ar_w_g_4s(t),int((ar_w_g_4s(t)/wordCount(t)*100))))
  print("Words > 12 Letters= {}\t{}%".format(w_g_12l(t),int((w_g_12l(t)/wordCount(t)*100))))
  print("############################################################Text Density Issues############################################################")
  print("Characters Per Word : {}".format(CPW(t)))
  print("Syllables Per Word : {}".format(ar_SPW(t)))
  print("Words Per Sentence : {}".format(WPS(t)))
  print("Words Per Paragraph : {}".format(WPP(t)))
  print("Sentences Per Paragraph : {}".format(SPP(t)))
  print("Spelling Issues: {}".format(ar_spellingIssues(t)))

t ="""
هَلْ ذَهَبْتَ إِلَى حَدِيقَةِ الْحَيَوَانِ؟ نَعَمْ، ذَهَبْتُ إَلَى حَدِيقَةِ الْحَيَوَانِ. لَا، لَمْ أَذْهَبْ إِلَى حَدِيقَةِ الْحَيَوَانِ.
مَنْ قَدَّمَ إِلَيْكَ الْهَدِيَّةَ؟ أَحْمَدُ صَدِيقِي هُوَ الَّذِي قَدَّمَ إِلَى الْهَدِيَّةَ.
مَا أَحَبُّ الْمَوَادِّ الدِّرَاسِيَّةِ إِلَيْكَ ؟ اللُّغَةُ الْعَرَبِيَّةُ أَحَبُّ الْمَوَادِّ إِلَيَّ.
مَتَى تَذْهَبُ إِلَى الشَّاطِئ؟ أَذْهَبُ إِلَى الشَّاطِئ فِي فَصْلِ الصَّيفِ.
أَيْنَ يُقَامُ الْحَفْلُ؟ يُقَامُ الْحَفْلُ فِي مَسْرَحِ الْمَدْرَسَةِ.
"""
ar_results(t)

from indicnlp.tokenize import sentence_tokenize
from indicnlp.tokenize import indic_tokenize 
import syllables
import math
def removePunctuation(text):
    result = ""
 
    for char in text:
        if char in (".",",","!","?","؟","،","\",""/","\"","#","$","%","&","'","(",")","*","+",":",";","<",">","=","[","]","^","_","`","{","}","|","~"):
            continue
        if char in ("-","\n","\r","\t"):
            char=" "
        result+=char
    return result

def characterCountHindi(text):
    #sentences=sentence_tokenize.sentence_split(text, lang='hi')
    count=0
    for t in indic_tokenize.trivial_tokenize(text):
        for i in t:
            count=count+1
    return count
    
def sentenceCountHindi(text):
    #text=removePunctuation(text)
    sentences=sentence_tokenize.sentence_split(text, lang='hi')
    return (len(sentences))

def wordCountHindi(text):
    text=removePunctuation(text)
    text=text.split()
    return(len(text))

def uniquewordCountHindi(text):
    count=0
    text=removePunctuation(text)
    words=text.split()
    uniqueWords=[]
    for i in words:
        if i not in uniqueWords:
            #print(i)
            uniqueWords.append(i)
    for word in uniqueWords:
        if word.isdigit():
            continue;
        count+=1
    return count

def syllableCountHindi(text):
    count=0
    text=removePunctuation(text)
    words=text.split()
    for i in words:
        count+=syllables.estimate(i)
    return count

def polysyllabicHindi(text):
    count=0
    num_syl=0
    text=removePunctuation(text)
    words=text.split()
    for i in words:
        num_syl=syllables.estimate(i)
        if(num_syl>=2):
            count+=1
    return count

def longWordCountHindi(text):
    words=indic_tokenize.trivial_tokenize(text)
    count=0
    for i in words:
        if len(i)>6:
            count+=1
    return count
def singleSyllableWordHindi(text):
    count=0
    num_syl=0
    text=removePunctuation(text)
    words=text.split()
    for i in words:
        num_syl=syllables.estimate(i)
        if(num_syl==1):
            count+=1
    return count
    

def paragraphCount(text):
    count=  [line.strip() for line in text.split('\n') if len(line.strip()) > 0]
    #print("Paragraph Count is",len(count))
    return len(count)

def FKRA(text):
    #txt = txt.lower()
    #txt=removePunctuation(txt)
    word = wordCountHindi(text)
    sentence =sentenceCountHindi(text)
    syllable =syllableCountHindi(text)
    ASL = float(word)/sentence
    ASW = float(syllable)/word
    FKRA = (0.39 * ASL) + (11.8 * ASW) - 15.59
    return round(FKRA,2)

def GFI(text):
    word = wordCountHindi(text)
    PSW=polysyllabicHindi(text)
    sentence =sentenceCountHindi(text)
    ASL = float(word)/sentence
    #syllable =syllableCount(txt)
 
    GFI = 0.4 * ( (ASL) + (PSW))
    return round(GFI,2)

def CLI(text):
    char = characterCountHindi(text)
    word = wordCountHindi(text)
    sentence =sentenceCountHindi(text)
    L = (float(char) / word) * 100
    S = (float(sentence) / word) * 100
    CLI = (0.0588 * L) - (0.296 * S) - 15.8
    return round(CLI,2)

def SMOGI(text):
    PSW=polysyllabicHindi(text)
    sentence =sentenceCountHindi(text)
    SMOGI = 1.0430 * (math.sqrt(PSW * (30 / float(sentence)))) + 3.1291
    return round(SMOGI,2)
 

def ARI(text):
    char = characterCountHindi(text)
    word = wordCountHindi(text)
    sentence =sentenceCountHindi(text)
    ARI = (4.71 * (float(char) / word)) + (.5 * (float(word) / sentence)- 21.43)
    return round(ARI , 2)
 
def FORCAST(text):
    syll=singleSyllableWordHindi(text)
    c_word = polysyllabicHindi(text)
    word = wordCountHindi(text)
    div=round(float(word*10)/150,2)
    #print(div)
    GL = 20 - (float(syll)/ div)
    return round(GL,2)
 
def PSKG(text):
    #txt=removePunctuation(txt)
    word = wordCountHindi(text)
    sentence =sentenceCountHindi(text)
    syllable =syllableCountHindi(text)
    ASL = float(word) / sentence
    #NS = (syllable * 0.0455) 
    #GL = (0.0778 * ASL) + NS + 2.7971
    NS = ((float(syllable)/word) * 0.0455*100) 
    GL = (0.0778 * ASL) + NS - 2.2029
    return round(GL,2)
 
def RIX(text):
    count = 0
    sentence =sentenceCountHindi(text)
    for x in text.split(" "):
        #print(characterCount(x))
        if characterCountHindi(text) > 4:
            count+=1
    return float(count) / sentence     
    


def FRE(txt):
    word = wordCountHindi(text)
    sentence =sentenceCountHindi(text)
    syllable =syllableCountHindi(text)
    ASL = float(word) / sentence
    ASW = float(syllable) / word
    RE=(0.39*ASL)+(11.8*ASW)-15.59
    #RE = 206.835 - (1.015 * ASL) - (84.6 * ASW)
    return round(RE,1)

def NDC(text):
    c_word = polysyllabicHindi(text)
    word = wordCountHindi(text)
    sentence =sentenceCountHindi(text)
    PDW = round(float(c_word) / word,2)*100
    ASL = float(word) / sentence
    if PDW > 5:
        RS = ((0.1579 * PDW) + (0.0496 * ASL)) + 3.6365
    else:
        RS = (0.1579 * PDW) + (0.0496 * ASL)
    return round(RS,1)
 
def SPACHE(text):
    c_word = polysyllabicHindi(text)
    word = wordCountHindi(text)
    sentence =sentenceCountHindi(text)
    PDW = round(float(c_word) / word,2)*100
    ASL = float(word) / sentence
    SP = ((0.141 *ASL) + (0.086 * PDW)+0.839)
    return round(SP,1)

def LIX(text):
    word = wordCountHindi(text)
    sentence =sentenceCountHindi(text)
    LongWords=longWordCountHindi(text)
    percentageOfLongWords=(float(LongWords)/word)*100
    avgLengthOfSentence=float(word)/sentence
    result=round(percentageOfLongWords+avgLengthOfSentence,0)
    return result
 

def LensearWrite(text):
    text=removePunctuation(text)
    words=text.split()
    word = wordCountHindi(text)
    sentence =sentenceCountHindi(text)
    #hardword=3
    #easyword=1
    ratio=100/word
    hardword=ratio*3
    easyword=ratio*1
    Score=0
    for w in words:
        if syllables.estimate(w) <= 2:
            Score+=easyword
        if syllables.estimate(w) >=3:
            Score+=hardword
    preResult=(Score/sentence)
    if preResult > 20:
        result=round((float(preResult)/2),1)
    else:
        result=round(float(preResult-2)/2,1)

    return result


def s_g_30Hindi(text):
    count=0
    num_syl=0
    sentences=sentence_tokenize.sentence_split(text, lang='hi')
    for i in sentences:
        if(syllables.estimate(i)>30):
            count+=1
    return count
 
#Sentences > 20 Syllables
def s_g_20Hindi(text):
    count=0
    num_syl=0
    sentences=sentence_tokenize.sentence_split(text, lang='hi')
    for i in sentences:
        if(syllables.estimate(i)>20):
            count+=1
    return count   
 
#Words > 4 Syllables
def w_g_4Hindi(text):
    count =0
    words=text.split()
    for x in words:
        if(syllables.estimate(x)>4):
            count+=1
    return count 
 
#Words > 12 Letters
def w_g_12Hindi(text):
    count =0
    words=text.split()
    for x in words:
        #print(len(x))
        if len(x) > 12:
            count+=1
    return count 
 
# Characters Per Word
def CPWHindi(text):
    word = wordCountHindi(text)
    char=characterCountHindi(text)
    CharactersPerWord=round(float(char)/word,1)
    return CharactersPerWord
 
#Syllables Per Word
def SPWHindi(text):
    word = wordCountHindi(text)
    syllable =syllableCountHindi(text)
    SyllablesPerWord=round(float(syllable)/word,1)
    return SyllablesPerWord
 
#Words Per Sentence
def WPSHindi(text):
    word = wordCountHindi(text)
    sentence =sentenceCountHindi(text)
    WordsPerSentence=round(float(word)/sentence,1)
    return WordsPerSentence
 
#Words Per Paragraph
def WPPHindi(txt):
    word = wordCountHindi(text)
    paragraph= paragraphCount(text)
    WordsPerParagraph=round(float(word)/paragraph,1)
    return WordsPerParagraph
 
#Sentences Per Paragraph
def SPPHindi(txt):
    sentence =sentenceCountHindi(text)
    paragraph= paragraphCount(text)
    SentencePerParagraph=round(float(sentence)/paragraph,1)
    return SentencePerParagraph
"""
def w_g_6l(txt):
    count =0
    words=word_tokenize(txt)
    for x in words:
        #print(len(x))
        if len(x) > 6:
            count+=1
    return count 
"""

def spellCheck(text):
    count=0
    text=removePunctuation(text)
    words=word_tokenize(text)
    #print(words)
    for i in words:
        if(i.isdigit()):
            continue;
        spellcheck = TextBlob(i)
        b=str(spellcheck.correct())
        print(b)
        
    #print("Word Count is ",count)
    return count

text1="Hello mi nam is Rashi"
spellCheck(text1)
text="लोनपो गार तिब्बत के बत्तीसवें राजा सौनगवसेन गांपो के मंत्री थे। वे"
print("Character Count : ",characterCountHindi(text))
print("Syllable Count: ",syllableCountHindi(text))
print("Word Count: ", wordCountHindi(text))
print("Unique Word Count: ",uniquewordCountHindi(text))
print("Sentence Count: ",sentenceCountHindi(text))
print("Paragraph Count: ",paragraphCount(text))

print("Flesch-Kincaid Grade Level : ", FKRA(text))
print("Gunning Fog Index : ",GFI(text))
print("Coleman–Liau index : ",CLI(text))
print("SMOG Index : ",SMOGI(text))
print("Automated Readability Index : ",ARI(text))
print("FORCAST Grade Level : ",FORCAST(text))
print("Powers Sumner Kearl Grade : ",PSKG(text))
print("RIX Readability : ",RIX(text))
print("Flesch Reading Ease : ",FRE(text))
print("Spache Score : ",SPACHE(text))
print("New Dale-Chall Score : ",NDC(text))
print("Lix Readability: ",LIX(text))
print("Lensear Write: ",LensearWrite(text))

print("Sentences > 30 Syllables = {}\t{}%".format(s_g_30Hindi(text),int((s_g_30Hindi(text)/sentenceCountHindi(text)*100))))
print("Sentences > 20 Syllables = {}\t{}%".format(s_g_20Hindi(text),int((s_g_20Hindi(text)/sentenceCountHindi(text)*100))))
print("Words > 4 Syllables= {}\t{}%".format(w_g_4Hindi(text),int((w_g_4Hindi(text)/wordCountHindi(text)*100))))
print("Words > 12 Letters= {}\t{}%".format(w_g_12Hindi(text),int((w_g_12Hindi(text)/wordCountHindi(text)*100))))

print("Characters Per Word : ",CPWHindi(text))
print("Syllables Per Word : ",SPWHindi(text))
print("Words Per Sentence : ",WPSHindi(text))
print("Words Per Paragraph : ",WPPHindi(text))
print("Sentences Per Paragraph : ",SPPHindi(text))

ta="""
The biodiversity and the biodensity in the ocean is higher, in places, than it is in the rainforests. It's mostly unexplored, and yet there are beautiful sights like this that captivate us and make us become familiar with it.
But when you're standing at the beach, I want you to think that you're standing at the edge of a very unfamiliar world.
We have to have a very special technology to get into that unfamiliar world.
"""

"""# Detect language"""

print("Enter your input here:")
X =input()
language = detect(X)
if language == 'en':
  print(X)
  print("***********************************************************")
  print("Language Detect is English")
  TextComposition=process_content(X)
  en_results(X)

elif language == 'ar':
  print(X)
  print("***********************************************************")
  print("Language Detect is Arabic")
  ar_results(X)
  # بِاسْتِخْدَامِ الشَّبَكَةِ الْمَعْلُومَاتِيَّةِ اجْمَعْ بَعْضَ الصُّوَرِ الْمُعَبِّرَةِ عَنِ الْأَمْرِ بِالْمَعْرُوفِ وَالنَّهْيِ عَنِ الْمُنْكَرِ فِي مُجْتَمَعِكَ



def allOutput(t):
  print(characterCount(t),"\t",syllableCount(t),"\t",wordCount(t),"\t",uniqueWordCount(t),"\t",sentenceCount(t),"\t",paragraphCount(t),"\t",ReadingTime(t),"\t",SpeakingTime(t),"\t",getAdjectivesCount(),"\t",getAdverbsCount(),"\t",getConjuctionsCount(),"\t",getDeterminersCount(),"\t",getInterjectionsCount(),"\t",getNounsCount(),"\t",getPropernounCount(),"\t",getPrepositionsCount(),"\t",getPronounsCount(),"\t-","\t",getVerbsCount(),"\t-","\t",nonWordCount(t),"\t-","\t",FKRA(t),"\t",
  GFI(t),"\t",CLI(t),"\t",SMOGI(t),"\t",ARI(t),"\t",FORCAST(t),"\t",PSKG(t),"\t",RIX(t),"\t-","\t-","\t",FRE(t),"\t-\t-\t",SPACHE(t),"\t",NDC(t),"\t",LIX(t),"\t",LensearWrite(t),"\t",spellingIssues(t),"\t",grammarIssues(t),"\t",s_g_30s(t),"\t",s_g_20s(t),"\t",
  w_g_4s(t),"\t",w_g_12l(t),"\t",passiveCount(t),"\t",getAdverbsCount(),"\t-","\t",CPW(t),"\t",SPW(t),"\t",WPS(t),"\t",WPP(t),"\t",SPP(t))
t="""
The biodiversity and the biodensity in the ocean is higher, in places, than it is in the rainforests. It's mostly unexplored, and yet there are beautiful sights like this that captivate us and make us become familiar with it.
But when you're standing at the beach, I want you to think that you're standing at the edge of a very unfamiliar world.
We have to have a very special technology to get into that unfamiliar world.
"""
TextComposition=process_content(t)
allOutput(t)





##########################################################################################333333
#spelling issues
"""
from spellchecker import SpellChecker
spell = SpellChecker() 
"""



#correct 
#We're going to see them more like this, as part of the productive, organic framework of which they are inevitably a part, symbiotically connected.

#worng
#t="""
#We're going to see them more likee this, as part of the productive, organica framework of which thei are inevitably a part, symbiotically connected.


"""
def take_data_to_shower(text):
  alphanumeric = ""
  for char in text:
    if char ==" ":
      alphanumeric += " "
      continue
    if char.isalnum():
      alphanumeric += char 
  return alphanumeric

def spellingIssues(t):
  text = take_data_to_shower(t)
  wrong =[]
  for w in (text.lower()).split():
    flag = str(w)in words.words()
    if flag == False:
      wrong.append(w)
  return len(wrong)
"""

"""
print("spelling issues count = {}".format(spellingIssues(t)))
print("Worng words:")
print(str(wrong)[1:-1])
print("correctness of worng words:")
for x in wrong:
  print("{} ==> {}".format(x,spell.correction(x)))
"""

################################################
#Grammer issues
tool = language_tool_python.LanguageTool('en-US')

#text = 'A sentence with a error in the Hitchhiker’s Guide tot he Galaxy'
"""
def grammarIsues(t):
  matches = tool.check(t)
  return (len(matches))
print("Grammer issues count : {}".format(len(matches)))
print("wrong is : ")
for i in matches:
  print("{}\n".format(i))
print("correct sentence is : {}".format(tool.correct(text)))
"""





