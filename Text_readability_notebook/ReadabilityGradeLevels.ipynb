{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "ReadabilityGradeLevels (2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfs2eX4ERs5x",
        "colab_type": "text"
      },
      "source": [
        "# import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Box3R3g3ZS6U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "c7ad53d1-e8e6-4bfb-99f1-10d857cdf6c4"
      },
      "source": [
        "!pip install syllables\n",
        "!pip install indic-nlp-library\n",
        "!pip install pyphen\n",
        "!pip install -U textblob\n",
        "!pip install pyspellchecker\n",
        "!pip install --upgrade language_tool_python\n",
        "import language_tool_python\n",
        "import nltk\n",
        "from nltk.corpus import words\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.corpus import cmudict\n",
        "nltk.download('words')\n",
        "nltk.download('cmudict')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from textblob import TextBlob\n",
        "import math\n",
        "import re\n",
        "import string\n",
        "import syllables\n",
        "d = cmudict.dict()\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker() \n",
        "! pip install langdetect\n",
        "from langdetect import detect\n",
        "import requests, time\n",
        "url = 'https://farasa-api.qcri.org'\n",
        "import ast"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: syllables in /usr/local/lib/python3.6/dist-packages (0.1.0)\n",
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.0.5)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->indic-nlp-library) (1.12.0)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.6/dist-packages (0.9.5)\n",
            "Requirement already up-to-date: textblob in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied, skipping upgrade: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from textblob) (3.2.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.1->textblob) (1.12.0)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.4)\n",
            "Requirement already up-to-date: language_tool_python in /usr/local/lib/python3.6/dist-packages (2.2.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from language_tool_python) (4.41.1)\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXHOUXWbR-j-",
        "colab_type": "text"
      },
      "source": [
        "# All Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk4Zwa8dY0b1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "#my source is: https://readabilityformulas.com/the-LIX-readability-formula.php\n",
        "def w_g_4(txt):\n",
        "    count =0\n",
        "    words=word_tokenize(txt)\n",
        "    for x in words:\n",
        "        #print(len(x))\n",
        "        if len(x) > 4:\n",
        "            count+=1\n",
        "    return count \n",
        " \n",
        "def LIX(txt):\n",
        "  WordCount=wordCount(txt)\n",
        "  SentenceCount=sentenceCount(txt)\n",
        "  LongWords=w_g_4(txt)\n",
        "  percentageOfLongWords=(LongWords/WordCount)*100\n",
        "  avgLengthOfSentence=WordCount/SentenceCount\n",
        "  result=round(percentageOfLongWords+avgLengthOfSentence,0)\n",
        "  return result\n",
        " \n",
        " \n",
        "def LensearWrite(text):\n",
        "  text=removePunctuation(text)\n",
        "  words=word_tokenize(text)\n",
        "  SentenceCount=sentenceCount(text)\n",
        "  wordcount=wordCount(text)\n",
        "  #hardword=3\n",
        "  #easyword=1\n",
        "  ratio=100/wordcount\n",
        "  hardword=ratio*3\n",
        "  easyword=ratio*1\n",
        "  Score=0\n",
        "  for word in words:\n",
        "    if nsyl(word) <= 2:\n",
        "      Score+=easyword\n",
        "    if nsyl(word) >=3:\n",
        "      Score+=hardword\n",
        " \n",
        "  #preResult=((Score/SentenceCount)/wordcount)*100\n",
        "  preResult=(Score/SentenceCount)\n",
        "  print(preResult)\n",
        "  #print(wordCount(text))\n",
        "  if preResult > 20:\n",
        "    result=round((preResult/2),1)\n",
        "  else:\n",
        "    result=round((preResult-2)/2,1)\n",
        "    \n",
        "  return result\n",
        " \n",
        "  \n",
        "print(LIX(t))\n",
        "print(LensearWrite(t))\n",
        " \n",
        " \n",
        "\"\"\"\n",
        " \n",
        " \n",
        " \n",
        " \n",
        "def removePunctuation(text):\n",
        "    result = \"\"\n",
        " \n",
        "    for char in text:\n",
        "        if char in (\".\",\",\",\"!\",\"?\",\"؟\",\"،\",\"\\\",\"\"/\",\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\":\",\";\",\"<\",\">\",\"=\",\"[\",\"]\",\"^\",\"_\",\"`\",\"{\",\"}\",\"|\",\"~\"):\n",
        "            continue\n",
        "        if char in (\"-\",\"\\n\",\"\\r\",\"\\t\"):\n",
        "            char=\" \"\n",
        "        result+=char\n",
        "    return result\n",
        " \n",
        "def removeSpace(text):\n",
        "    result=\"\"\n",
        "    for char in text:\n",
        "        if char in (\"-\",\"\\n\",\"\\r\",\"\\t\"):\n",
        "            char=\" \"\n",
        "        result+=char\n",
        "    return result\n",
        " \n",
        "def removeDigits(text):\n",
        "    result=\"\"\n",
        "    for i in text:\n",
        "        if(i.isdigit()):\n",
        "            continue\n",
        "        result+=i\n",
        "    return result\n",
        " \n",
        "def characterCount(text):\n",
        "    count=0\n",
        "    text=removePunctuation(text)\n",
        "    text = re.sub('[ًٌٍَُِّْـ]+', '', text)\n",
        "    for char in text:\n",
        "        if (char.isdigit() or char.isspace()):\n",
        "            continue;\n",
        "        count+=1\n",
        "    #print(\"Character Count is \",count)\n",
        "    return count\n",
        " \n",
        "def wordCount(text):\n",
        "    count=0\n",
        "    text=removePunctuation(text)\n",
        "    words=word_tokenize(text)\n",
        "    #print(words)\n",
        "    for i in words:\n",
        "        if(i.isdigit()):\n",
        "            continue;\n",
        "        count+=1\n",
        "    #print(\"Word Count is \",count)\n",
        "    return count\n",
        "   \n",
        "def count_complex_word(text):\n",
        "    complex_words = 0\n",
        "    for i in text.split(' '):        \n",
        "        syllable_count = syllables(i)\n",
        "        if (syllable_count >= 3):\n",
        "            complex_words +=1\n",
        "    #print(\"Complex Word Count is \",complex_words)\n",
        "    return max(1, complex_words)\n",
        " \n",
        "def nsyl(word):\n",
        "    try:\n",
        "        #print(d[word.lower()])\n",
        "        li=[]\n",
        "        #print(word,\"-\",d[word.lower()])\n",
        "        for x in d[word.lower()][0]:\n",
        "            for y in x:\n",
        "                #print(y)\n",
        "                if(y[-1].isdigit()):\n",
        "                    li.append(y)\n",
        "        #print(word,\":\",len(li))\n",
        "        return(len(li))\n",
        "    except:\n",
        "        #if word not found in cmudict\n",
        "        return(syllables(word))\n",
        " \n",
        "def syllables(word):\n",
        "    count = 0\n",
        "    vowels = 'aeiouy'\n",
        "    word = word.lower()\n",
        "    if word[0] in vowels:\n",
        "        count +=1\n",
        "    for index in range(1,len(word)):\n",
        "        if word[index] in vowels:\n",
        "            if word[index-1] not in vowels:\n",
        "                count +=1\n",
        "    if word.endswith('e'):\n",
        "        count -= 1\n",
        "    if word.endswith('le'):\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    #print(\"Count:\",count)\n",
        "    return count\n",
        " \n",
        "def syllableCount(text):\n",
        "    count=0\n",
        "    num=0\n",
        "    text=removePunctuation(text)\n",
        "    words=word_tokenize(text)\n",
        "    #print(words)\n",
        "    for i in words:\n",
        "        count=count+nsyl(i)\n",
        "    return(count)\n",
        "       \n",
        "def singleSyllableWord(text):\n",
        "    text=removePunctuation(text)\n",
        "    text=removeDigits(text)\n",
        "    words=word_tokenize(text)\n",
        "    result=0\n",
        "    for i in words:\n",
        "        count=0\n",
        "        count=nsyl(i)\n",
        "        if(count==1):\n",
        "            #print(i)\n",
        "            result=result+1\n",
        "    #print(result)\n",
        "    #print(\"Single Syllable Words are: \",result)\n",
        "    return result\n",
        " \n",
        "def uniqueWordCount(text):\n",
        "    count=0\n",
        "    text=removePunctuation(text)\n",
        "    text=text.lower()\n",
        "    words=word_tokenize(text)\n",
        "    uniqueWords=[]\n",
        "    for i in words:\n",
        "        if i not in uniqueWords:\n",
        "            #print(i)\n",
        "            uniqueWords.append(i)\n",
        "    for word in uniqueWords:\n",
        "        if word.isdigit():\n",
        "            continue;\n",
        "        count+=1\n",
        "    #print(\"Unique Count is \",count)\n",
        "    return count\n",
        "   \n",
        "\"\"\"def sentenceCount(text):\n",
        "    count = 0 \n",
        "    for i in text:\n",
        "      if i in(\".\",\"?\",\"!\",\"؟\"):\n",
        "        count+=1\n",
        "    return max(1,count)\"\"\"\n",
        "\n",
        "def sentenceCount(text):\n",
        "  count = 0\n",
        "  text2 = text + \"\\n\"\n",
        "  text2 = text2.replace(\".\",\"\\n\")\n",
        "  text2 = text2.replace(\"!\",\"\\n\")\n",
        "  text2 = text2.replace(\"?\",\"\\n\")\n",
        "  text2 = text2.replace(\"؟\",\"\\n\")\n",
        "  lines = text2.split(\"\\n\")\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if len(line) == 0:\n",
        "      continue\n",
        "    count+=1   \n",
        "    if count == 0:\n",
        "      count = 1 \n",
        "  return count     \n",
        "def paragraphCount(text):\n",
        "    count=  [line.strip() for line in text.split('\\n') if len(line.strip()) > 0]\n",
        "    #print(\"Paragraph Count is\",len(count))\n",
        "    return len(count)\n",
        " \n",
        "def ReadingTime(text):\n",
        "    word = wordCount(text)\n",
        "    div_wordCount = word / 225\n",
        "    second,minute=math.modf(div_wordCount)\n",
        "    if second >= 0.60:\n",
        "      minute +=1\n",
        "      second-=0.60\n",
        "    #second=round((second*0.60),2)\n",
        "    #print(\"Reading Time is\",minute+second);\n",
        "    return int(minute),int(round(second,2)*100)\n",
        " \n",
        "def SpeakingTime(text):\n",
        "    word = wordCount(text)\n",
        "    speakingRate=125\n",
        "    time=word/speakingRate\n",
        "    second,minute=math.modf(time)\n",
        "    if second >= 0.60:\n",
        "      minute +=1\n",
        "      second-=0.60\n",
        "    #second=round((second*0.60),2)\n",
        "    #print(\"Reading Time is\",minute+second);\n",
        "    return int(minute),int(round(second,2)*100)\n",
        " \n",
        "def FKRA(txt):\n",
        "    #txt = txt.lower()\n",
        "    #txt=removePunctuation(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence =sentenceCount(txt)\n",
        "    syllable =syllableCount(txt) \n",
        "    ASL = float(word)/sentence\n",
        "    ASW = float(syllable)/word\n",
        "    FKRA = (0.39 * ASL) + (11.8 * ASW) - 15.59\n",
        "    return round(FKRA,2)\n",
        " \n",
        "def GFI(txt):\n",
        "    #txt=removePunctuation(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence = sentenceCount(txt)\n",
        "    #syllable =syllableCount(txt)\n",
        "    c_word = count_complex_word(txt)\n",
        " \n",
        "    GFI = 0.4 * ((float(word) / sentence) + 100 * (float(c_word) / word))\n",
        "    return round(GFI,2)\n",
        " \n",
        "def CLI(txt):\n",
        "    txt = txt.lower()\n",
        "    char = characterCount(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence = sentenceCount(txt)\n",
        "    L = (float(char) / word) * 100\n",
        "    S = (float(sentence) / word) * 100\n",
        "    CLI = (0.0588 * L) - (0.296 * S) - 15.8\n",
        "    return round(CLI,2)\n",
        " \n",
        "def SMOGI(txt):\n",
        "    txt = txt.lower()\n",
        "    sentence = sentenceCount(txt)\n",
        "    plosyllable = count_complex_word(txt)\n",
        "    SMOGI = 1.0430 * (math.sqrt(plosyllable * (30 / float(sentence)))) + 3.1291\n",
        "    return round(SMOGI,2)\n",
        " \n",
        "def ARI(txt):\n",
        "    txt = txt.lower()\n",
        "    char =characterCount(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence =sentenceCount(txt)\n",
        "    ARI = (4.71 * (float(char) / word)) + (.5 * (float(word) / sentence)- 21.43)\n",
        "    return round(ARI , 2)\n",
        " \n",
        "def FORCAST(txt):\n",
        "    syll=singleSyllableWord(txt)\n",
        "    c_word = count_complex_word(txt)\n",
        "    div=round((float(wordCount(txt))*10)/150,2)\n",
        "    #print(div)\n",
        "    GL = 20 - (float(syll)/ div)\n",
        "    return round(GL,2)\n",
        " \n",
        "def PSKG(txt):\n",
        "    #txt=removePunctuation(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence = sentenceCount(txt)\n",
        "    syllable = syllableCount(txt)\n",
        "    ASL = float(word) / sentence\n",
        "    #NS = (syllable * 0.0455) \n",
        "    #GL = (0.0778 * ASL) + NS + 2.7971\n",
        "    NS = ((float(syllable)/word) * 0.0455*100) \n",
        "    GL = (0.0778 * ASL) + NS - 2.2029\n",
        "    return round(GL,2)\n",
        " \n",
        " \n",
        "def RIX(txt):\n",
        "    count = 0\n",
        "    sentence = sentenceCount(txt)\n",
        "    for x in txt.split(\" \"):\n",
        "        #print(characterCount(x))\n",
        "        if characterCount(x) > 4:\n",
        "            count+=1\n",
        "    return round(float(count) / sentence,2)   \n",
        " \n",
        "def FRE(txt):\n",
        "    sentence = sentenceCount(txt)\n",
        "    word = wordCount(txt)\n",
        "    syllable = syllableCount(txt)\n",
        "    #print(word)\n",
        "    #print(sentence)\n",
        "    #print(syllable)\n",
        "    ASL = float(word) / sentence\n",
        "    ASW = float(syllable) / word\n",
        "    RE = 206.835 - (1.015 * ASL) - (84.6 * ASW)\n",
        "    return round(RE,1)\n",
        " \n",
        "def NDC(txt):\n",
        "    c_word = count_complex_word(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence = sentenceCount(txt)\n",
        "    PDW = round(float(c_word) / word,2)*100\n",
        "    ASL = float(word) / sentence\n",
        "    if PDW > 5:\n",
        "        RS = ((0.1579 * PDW) + (0.0496 * ASL)) + 3.6365\n",
        "    else:\n",
        "        RS = (0.1579 * PDW) + (0.0496 * ASL)\n",
        "    return round(RS,1)\n",
        " \n",
        "def SPACHE(txt):\n",
        "    c_word = count_complex_word(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence = sentenceCount(txt)\n",
        "    PDW = round(float(c_word) / word,2)*100\n",
        "    ASL = float(word) / sentence\n",
        "    SP = ((0.141 *ASL) + (0.086 * PDW)+0.839)\n",
        "    return round(SP,1)  \n",
        " \n",
        "#Sentences > 30 Syllables\n",
        "def s_g_30s(txt):\n",
        "    count =0\n",
        "    sentence=sent_tokenize(txt)\n",
        "    for x in sentence:\n",
        "        #print(syllableCount(x))\n",
        "        if syllableCount(x) > 30:\n",
        "            count+=1\n",
        "    return count    \n",
        " \n",
        "#Sentences > 20 Syllables\n",
        "def s_g_20s(txt):\n",
        "    count =0\n",
        "    sentence=sent_tokenize(txt)\n",
        "    for x in sentence:\n",
        "        #print(syllableCount(x))\n",
        "        if syllableCount(x) > 20:\n",
        "            count+=1\n",
        "    return count    \n",
        " \n",
        "#Words > 4 Syllables\n",
        "def w_g_4s(txt):\n",
        "    count =0\n",
        "    words=word_tokenize(txt)\n",
        "    for x in words:\n",
        "        #print(syllableCount(x))\n",
        "        if syllableCount(x) >= 4:\n",
        "            count+=1\n",
        "    return count  \n",
        " \n",
        "#Words > 12 Letters\n",
        "def w_g_12l(txt):\n",
        "    count =0\n",
        "    words=word_tokenize(txt)\n",
        "    for x in words:\n",
        "        #print(len(x))\n",
        "        if len(x) > 12:\n",
        "            count+=1\n",
        "    return count \n",
        " \n",
        "# Characters Per Word\n",
        "def CPW(txt):\n",
        "  WordCount=wordCount(txt)\n",
        "  CharacterCount=characterCount(txt)\n",
        "  CharactersPerWord=round(float(CharacterCount)/WordCount,1)\n",
        "  return CharactersPerWord\n",
        " \n",
        "#Syllables Per Word\n",
        "def SPW(txt):\n",
        "  WordCount=wordCount(txt)\n",
        "  SyllableCount= syllableCount(txt)\n",
        "  SyllablesPerWord=round(float(SyllableCount)/WordCount,1)\n",
        "  return SyllablesPerWord\n",
        " \n",
        "#Words Per Sentence\n",
        "def WPS(txt):\n",
        "  WordCount=wordCount(txt)\n",
        "  SentenceCount=sentenceCount(txt)\n",
        "  WordsPerSentence=round(float(WordCount)/SentenceCount,1)\n",
        "  return WordsPerSentence\n",
        " \n",
        "#Words Per Paragraph\n",
        "def WPP(txt):\n",
        "  WordCount=wordCount(txt)\n",
        "  ParagraphCount=paragraphCount(txt)\n",
        "  WordsPerParagraph=round(float(WordCount)/ParagraphCount,1)\n",
        "  return WordsPerParagraph\n",
        " \n",
        "#Sentences Per Paragraph\n",
        "def SPP(txt):\n",
        "  SentenceCount=sentenceCount(txt)\n",
        "  ParagraphCount=paragraphCount(txt)\n",
        "  SentencePerParagraph=round(float(SentenceCount)/ParagraphCount,1)\n",
        "  return SentencePerParagraph\n",
        " \n",
        "def w_g_6l(txt):\n",
        "    count =0\n",
        "    words=word_tokenize(txt)\n",
        "    for x in words:\n",
        "        #print(len(x))\n",
        "        if len(x) > 6:\n",
        "            count+=1\n",
        "    return count \n",
        " \n",
        "def LIX(txt):\n",
        "  WordCount=wordCount(txt)\n",
        "  SentenceCount=sentenceCount(txt)\n",
        "  LongWords=w_g_6l(txt)\n",
        "  percentageOfLongWords=(float(LongWords)/WordCount)*100\n",
        "  avgLengthOfSentence=float(WordCount)/SentenceCount\n",
        "  result=round(percentageOfLongWords+avgLengthOfSentence,0)\n",
        "  return result\n",
        " \n",
        "def process_content(text):\n",
        "  \n",
        "    dict_group={\"Adjectives\":[\"JJ\",\"JJR\",\"JJS\"],\"Adverbs\":[\"RB\",\"RBR\",\"RBS\"],\"Conjuctions\":[\"CC\"],\"Determiners\":[\"DT\",\"PDT\",\"WDT\"],\"Interjections\":[\"UH\"],\"Nouns\":[\"NN\",\"NNS\"],\"Proper Nouns\":[\"NNP\",\"NNPS\"],\"Prepositions\":[\"IN\"],\"Pronouns\":[\"PRP\",\"PRP$\",\"WP\",\"WP$\"],\"Verbs\":[\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"],}\n",
        "    dict_count={\"Adjectives\":0,\"Adverbs\":0,\"Conjuctions\":0,\"Determiners\":0,\"Interjections\":0,\"Nouns\":0,\"Proper Nouns\":0,\"Prepositions\":0,\"Pronouns\":0,\"Verbs\":0}\n",
        "    #dict_group={\"Adjectives\":[\"JJ\",\"JJR\",\"JJS\"],\"Adverbs\":[\"RB\",\"RBR\",\"RBS\"],\"Conjuctions\":[\"CC\"],\"Determiners\":[\"PDT\",\"WDT\"],\"Interjections\":[\"UH\"],\"Nouns\":[\"NN\",\"NNS\"],\"Proper Nouns\":[\"NNP\",\"NNPS\"],\"Prepositions\":[\"IN\"],\"Pronouns\":[\"PRP\",\"PRP$\",\"WP\",\"WP$\"],\"Verbs\":[\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"],}\n",
        "    #dict_count={\"Adjectives\":0,\"Adverbs\":0,\"Conjuctions\":0,\"Determiners\":0,\"Interjections\":0,\"Nouns\":0,\"Proper Nouns\":0,\"Prepositions\":0,\"Pronouns\":0,\"Verbs\":0}\n",
        "    tokenized=sent_tokenize(text)\n",
        "    try:\n",
        "        for tokens in tokenized:\n",
        "            words = nltk.word_tokenize(tokens)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            for word in tagged:\n",
        "                pos=word[1]\n",
        "                for listKey,listElem in dict_group.items():    \n",
        "                    if pos in listElem:\n",
        "                        dict_count[listKey]+=1\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "    #Final Result Printed.\n",
        "    #print(dict_count)\n",
        "    TextComposition=list(dict_count.items())\n",
        "    return(TextComposition)\n",
        "  \n",
        "def getAdjectivesCount():\n",
        "  return TextComposition[0][1]\n",
        " \n",
        "def getAdverbsCount():\n",
        "  return TextComposition[1][1]\n",
        " \n",
        "def getConjuctionsCount():\n",
        "  return TextComposition[2][1]\n",
        " \n",
        "def getDeterminersCount():\n",
        "  print(\"Answer-\",TextComposition[3][1])\n",
        "  return TextComposition[3][1]\n",
        " \n",
        "def getInterjectionsCount():\n",
        "  return TextComposition[4][1]\n",
        " \n",
        "def getNounsCount():\n",
        "  return TextComposition[5][1]\n",
        " \n",
        "def getPropernounCount():\n",
        "  return TextComposition[6][1]\n",
        " \n",
        "def getPrepositionsCount():\n",
        "  return TextComposition[7][1]\n",
        " \n",
        "def getPronounsCount():\n",
        "  return TextComposition[8][1]\n",
        " \n",
        "def getVerbsCount():\n",
        "  return TextComposition[9][1]\n",
        " \n",
        "def nonWordCount(text):\n",
        "    regex=\"[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w+$\"    #checks for email address\n",
        "    #text=removePunctuation(text)\n",
        "    emails = re.findall(\"([a-zA-Z0-9]+[\\._]?@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\", text)\n",
        "    print(\"Non word: \",emails)\n",
        "    return(len(emails))\n",
        " \n",
        "def LensearWrite(text):\n",
        "  text=removePunctuation(text)\n",
        "  words=word_tokenize(text)\n",
        "  SentenceCount=sentenceCount(text)\n",
        "  wordcount=wordCount(text)\n",
        "  #hardword=3\n",
        "  #easyword=1\n",
        "  ratio=100/float(wordcount)\n",
        "  hardword=ratio*3\n",
        "  easyword=ratio*1\n",
        "  Score=0\n",
        "  for word in words:\n",
        "    if nsyl(word) <= 2:\n",
        "      Score+=easyword\n",
        "    if nsyl(word) >=3:\n",
        "      Score+=hardword\n",
        "  preResult=(Score/float(SentenceCount))\n",
        "  if preResult > 20:\n",
        "    result=round((preResult/2),1)\n",
        "  else:\n",
        "    result=round((preResult-2)/2,1)\n",
        "    \n",
        "  return result\n",
        " \n",
        "def take_data_to_shower(text):\n",
        "    text=removeSpace(text)\n",
        "    alphanumeric = \"\"\n",
        "    for char in text:\n",
        "        if char ==\" \":\n",
        "            alphanumeric += \" \"\n",
        "            continue\n",
        "        if char.isalnum():\n",
        "            alphanumeric += char \n",
        "    #print(alphanumeric)\n",
        "    return alphanumeric\n",
        " \n",
        "def spellingIssues(t):\n",
        "    text = take_data_to_shower(t)\n",
        "    wrong =[]\n",
        "    for w in (text.lower()).split():\n",
        "        flag = str(w)in words.words()\n",
        "        if flag == False and w not in wrong:\n",
        "            wrong.append(w)\n",
        "    return len(wrong)\n",
        "tool = language_tool_python.LanguageTool('en-US')\n",
        " \n",
        "def grammarIssues(t):\n",
        "  matches = tool.check(t)\n",
        "  return (len(matches))\n",
        " \n",
        "def isPassive(sentence):\n",
        "    beforms = ['am', 'is', 'are', 'been', 'was', 'were', 'be', 'being']              \n",
        "    aux = ['do', 'did', 'does', 'have', 'has', 'had']                                 \n",
        "    words = nltk.word_tokenize(sentence)\n",
        "    tokens = nltk.pos_tag(words)\n",
        "    tags = [i[1] for i in tokens]\n",
        "    if tags.count('VBN') == 0:                                                           \n",
        "        return False\n",
        "    elif tags.count('VBN') == 1 and 'been' in words:                                   \n",
        "        return False\n",
        "    else:\n",
        "        pos = [i for i in range(len(tags)) if tags[i] == 'VBN' and words[i] != 'been']  \n",
        "        end=pos[0]\n",
        "        chunk = tags[:end]\n",
        "        start = 0\n",
        "        for i in range(len(chunk), 0, -1):\n",
        "            last = chunk.pop()\n",
        "            if last == 'NN' or last == 'PRP':\n",
        "                start = i                                                             \n",
        "                break\n",
        "        sentchunk = words[start:end]\n",
        "        tagschunk = tags[start:end]\n",
        "        verbspos = [i for i in range(len(tagschunk)) if tagschunk[i].startswith('V')] \n",
        "        if verbspos != []:                                                           \n",
        "            for i in verbspos:\n",
        "                if sentchunk[i].lower() not in beforms and sentchunk[i].lower() not in aux:  \n",
        "                    break\n",
        "            else:\n",
        "                return True\n",
        "    return False\n",
        " \n",
        "def passiveCount(text):\n",
        "    sentences= nltk.sent_tokenize(text)\n",
        "    count=0\n",
        "    for sent in sentences:\n",
        "        val=isPassive(sent)\n",
        "        if(val==True):\n",
        "            count+=1\n",
        "    return count\n",
        " \n",
        " \n",
        "def en_results(t):\n",
        " \n",
        "  print(\"############################################################Text Statistics############################################################\")\n",
        "  print(\"Character Count : {}\".format(characterCount(t)))\n",
        "  print(\"Syllable Count : {}\".format(syllableCount(t)))\n",
        "  print(\"Word Count : {}\".format(wordCount(t)))\n",
        "  print(\"Unique Word Count : {}\".format(uniqueWordCount(t)))\n",
        "  print(\"Sentence Count : {}\".format(sentenceCount(t)))\n",
        "  print(\"Paragraph Count : {}\".format(paragraphCount(t)))\n",
        "  print(\"Reading Time: {}\".format(ReadingTime(t)))\n",
        "  print(\"Speaking Time: {}\".format(SpeakingTime(t)))\n",
        "  print(\"############################################################Text Composition############################################################\")\n",
        "  print(\"Adjectives : {}\".format(getAdjectivesCount()))\n",
        "  print(\"Adverbs : {}\".format(getAdverbsCount()))\n",
        "  print(\"Conjuctions : {}\".format(getConjuctionsCount()))\n",
        "  print(\"Determiners : {}\".format(getDeterminersCount()))\n",
        "  print(\"Interjections : {}\".format(getInterjectionsCount()))\n",
        "  print(\"Nouns : {}\".format(getNounsCount()))\n",
        "  print(\"Proper Nouns : {}\".format(getPropernounCount()))\n",
        "  print(\"Prepositions : {}\".format(getPrepositionsCount()))\n",
        "  print(\"Pronouns : {}\".format(getPronounsCount()))\n",
        "  print(\"Verbs : {}\".format(getVerbsCount()))\n",
        "  print(\"############################################################Readability Grade Levels############################################################\")\n",
        "  print(\"Flesch-Kincaid Grade Level : {}\".format(FKRA(t)))\n",
        "  print(\"Gunning Fog Index : {}\".format(GFI(t)))\n",
        "  print(\"Coleman–Liau index : {}\".format(CLI(t)))\n",
        "  print(\"SMOG Index : {}\".format(SMOGI(t)))\n",
        "  print(\"Automated Readability Index : {}\".format(ARI(t)))\n",
        "  print(\"FORCAST Grade Level : {}\".format(FORCAST(t)))\n",
        "  print(\"Powers Sumner Kearl Grade : {}\".format(PSKG(t)))\n",
        "  print(\"RIX Readability : {}\".format(RIX(t)))\n",
        "  print(\"############################################################Readability Scores############################################################\")\n",
        "  print(\"Flesch Reading Ease : {}\".format(FRE(t)))\n",
        "  print(\"Spache Score : {}\".format(SPACHE(t)))\n",
        "  print(\"New Dale-Chall Score : {}\".format(NDC(t)))\n",
        "  print(\"Lix Readability: {}\".format(LIX(t)))\n",
        "  print(\"Lensear Write: {}\".format(LensearWrite(t)))\n",
        "  print(\"############################################################Readability Issues############################################################\")\n",
        "  print(\"Sentences > 30 Syllables = {}\\t{}%\".format(s_g_30s(t),int((s_g_30s(t)/sentenceCount(t)*100))))\n",
        "  print(\"Sentences > 20 Syllables = {}\\t{}%\".format(s_g_20s(t),int((s_g_20s(t)/sentenceCount(t)*100))))\n",
        "  print(\"Words > 4 Syllables= {}\\t{}%\".format(w_g_4s(t),int((w_g_4s(t)/wordCount(t)*100))))\n",
        "  print(\"Words > 12 Letters= {}\\t{}%\".format(w_g_12l(t),int((w_g_12l(t)/wordCount(t)*100))))\n",
        "  print(\"############################################################Text Density Issues############################################################\")\n",
        "  print(\"Characters Per Word : {}\".format(CPW(t)))\n",
        "  print(\"Syllables Per Word : {}\".format(SPW(t)))\n",
        "  print(\"Words Per Sentence : {}\".format(WPS(t)))\n",
        "  print(\"Words Per Paragraph : {}\".format(WPP(t)))\n",
        "  print(\"Sentences Per Paragraph : {}\".format(SPP(t)))\n",
        "  print(\"Grammar Issues : {}\".format(grammarIssues(t)))\n",
        "  print(\"Spelling Issues: {}\".format(spellingIssues(t)))\n",
        "  print(\"Passive Voice Count : {}\".format(passiveCount(t))) \n",
        " \n",
        "#############################################################################################################################################\n",
        "                                                               #Arabic \n",
        "##############################################################################################################################################\n",
        " \n",
        "def Seg(text):\n",
        "  apiURL = \"/msa/webapi/segmenter\"\n",
        "  text = {'text': text}\n",
        "  headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\" }\n",
        "  response = requests.get(url + apiURL, params=text, headers=headers)\n",
        "  result =  response.text\n",
        "  res = ast.literal_eval(result)\n",
        "  return res[\"segtext\"]\n",
        " \n",
        " \n",
        " \n",
        "def lemma(text):\n",
        "  apiURL = \"/msa/webapi/lemma\"\n",
        "  text = {'text': text}\n",
        "  headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\" }\n",
        "  response = requests.get(url + apiURL, params=text, headers=headers)\n",
        "  result =  response.text\n",
        "  res = ast.literal_eval(result)\n",
        "  return res[\"result\"]\n",
        "  \n",
        " \n",
        "def POS(text):\n",
        "  apiURL = \"/msa/webapi/pos\"\n",
        "  text = {'text': text}\n",
        "  headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\" }\n",
        "  response = requests.get(url + apiURL, params=text, headers=headers)\n",
        "  result = response.text\n",
        "  res = ast.literal_eval(result)\n",
        "  arr=[] \n",
        "  for x in range(1,len(res)-1):\n",
        "      r=res[x]\n",
        "      arr.append(r[\"POS\"])\n",
        " \n",
        "  my_dict = {i:arr.count(i) for i in arr}\n",
        "  for keys,values in my_dict.items():\n",
        "      print(keys,\" :\",values)\n",
        " \n",
        "def SpCH(text):\n",
        "  apiURL = \"/msa/webapi/spellcheck\"\n",
        "  text = {'text': text}\n",
        "  headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\" }\n",
        "  response = requests.get(url + apiURL, params=text, headers=headers)\n",
        "  result =  response.text\n",
        "  res = ast.literal_eval(result)\n",
        "  return res[\"result\"]\n",
        " \n",
        "def Diac(text):\n",
        "  apiURL = \"/msa/webapi/diacritize\"\n",
        "  text = {'text': text}\n",
        "  headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\" }\n",
        "  response = requests.get(url + apiURL, params=text, headers=headers)\n",
        "  result =  response.text\n",
        "  res = ast.literal_eval(result)\n",
        "  return res[\"output\"]\n",
        " \n",
        "def Diac2(text):\n",
        "  apiURL = \"/msa/webapi/diacritizeV2\"\n",
        "  text = {'text': text}\n",
        "  headers = { 'content-type': \"application/json\", 'cache-control': \"no-cache\" }\n",
        "  response = requests.get(url + apiURL, params=text, headers=headers)\n",
        "  result =  response.text\n",
        "  res = ast.literal_eval(result)\n",
        "  return res[\"output\"]\n",
        " \n",
        "def ar_syllables(word):\n",
        "    #fatha, damma, kasra\n",
        "    tashkeel = ['\\u064E','\\u064F','\\u0650']\n",
        " \n",
        "    count_long = 0\n",
        "    count_short = 0\n",
        "    count_stress = 0\n",
        " \n",
        "    for x in range(0,len(tashkeel)):\n",
        "        for i in range(0, len(word)):\n",
        "            if word[i] == tashkeel[x]:\n",
        "                if i+1 < len(word):\n",
        "                    #to count long syllables we need to check if the character following is an alef, waw or yaaA.\n",
        "                    if word[i+1]=='\\u0627' or word[i+1]=='\\u0648' or word[i+1]=='\\u064a':\n",
        "                        count_long += 1\n",
        "                    else:\n",
        "                        count_short += 1\n",
        "                else:\n",
        "                    count_short += 1\n",
        " \n",
        "    #counts stress syllables, those tanween fatih, tanween damm, tanween kasr and shadda.\n",
        "    count_stress = word.count(\"\\u064B\") + word.count(\"\\u064C\") + word.count(\"\\u064D\") + word.count(\"\\u0651\")\n",
        "    #syllables_count = count_short + (2 * count_long) + (2 * count_stress)\n",
        "    syllables_count = count_short +  count_long +  count_stress\n",
        "    return syllables_count\n",
        " \n",
        "def ar_syllables_count(text):\n",
        "  words = text.split(\" \")\n",
        "  count = 0\n",
        "  for i in words:\n",
        "    count = count+ar_syllables(i)\n",
        "  return count  \n",
        " \n",
        "def ar_singleSyllableWord(text):\n",
        "    words=word_tokenize(text)\n",
        "    result=0\n",
        "    for i in words:\n",
        "        count=0\n",
        "        count=ar_syllables(i)\n",
        "        if(count==1):\n",
        "            result=result+1\n",
        "    return result\n",
        " \n",
        "def ar_count_complex_word(text):\n",
        "    complex_words = 0\n",
        "    for i in text.split(' '):        \n",
        "        syllable_count = ar_syllables(i)\n",
        "        if (syllable_count >= 4):\n",
        "            complex_words +=1\n",
        "    #print(\"Complex Word Count is \",complex_words)\n",
        "    return max(1, complex_words)\n",
        " \n",
        "def ar_FKRA(txt):\n",
        "    word = wordCount(txt)\n",
        "    sentence =sentenceCount(txt)\n",
        "    syllable =ar_syllables_count(txt) \n",
        "    ASL = float(word)/sentence\n",
        "    ASW = float(syllable)/word\n",
        "    FKRA = (0.39 * ASL) + (11.8 * ASW) - 15.59\n",
        "    return round(FKRA,2)\n",
        " \n",
        "def ar_GFI(txt):\n",
        "    word = wordCount(txt)\n",
        "    sentence = sentenceCount(txt)\n",
        "    c_word = ar_count_complex_word(txt)\n",
        " \n",
        "    GFI = 0.4 * ((float(word) / sentence) + 100 * (float(c_word) / word))\n",
        "    return round(GFI,2)\n",
        " \n",
        "def ar_CLI(txt):\n",
        "    char = characterCount(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence = sentenceCount(txt)\n",
        "    L = (float(char) / word) * 100\n",
        "    S = (float(sentence) / word) * 100\n",
        "    CLI = (0.0588 * L) - (0.296 * S) - 15.8\n",
        "    return round(CLI,2)\n",
        " \n",
        "def ar_SMOGI(txt):\n",
        "    sentence = sentenceCount(txt)\n",
        "    plosyllable = ar_count_complex_word(txt)\n",
        "    SMOGI = 1.0430 * (math.sqrt(plosyllable * (30 / float(sentence)))) + 3.1291\n",
        "    return round(SMOGI,2)\n",
        " \n",
        "def ar_ARI(txt):\n",
        "    char =characterCount(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence =sentenceCount(txt)\n",
        "    ARI = (4.71 * (float(char) / word)) + (.5 * (float(word) / sentence)- 21.43)\n",
        "    return round(ARI , 2)\n",
        " \n",
        "def ar_FORCAST(txt):\n",
        "    syll=ar_singleSyllableWord(txt)\n",
        "    c_word = ar_count_complex_word(txt)\n",
        "    div=round( (wordCount(txt)*10)/150,2)\n",
        "    #print(div)\n",
        "    GL = 20 - (float(syll)/ div)\n",
        "    return round(GL,2)\n",
        " \n",
        "def ar_PSKG(txt):\n",
        "    word = wordCount(txt)\n",
        "    sentence = sentenceCount(txt)\n",
        "    syllable = ar_syllables_count(txt)\n",
        "    ASL = float(word) / sentence\n",
        "    NS = ((float(syllable)/word) * 0.0455*100) \n",
        "    GL = (0.0778 * ASL) + NS - 2.2029\n",
        "    return round(GL,2)\n",
        " \n",
        " \n",
        "def ar_RIX(txt):\n",
        "    count = 0\n",
        "    sentence = sentenceCount(txt)\n",
        "    for x in txt.split(\" \"):\n",
        "        if characterCount(x) > 4:\n",
        "            count+=1\n",
        "    return round(float(count) / sentence,2)  \n",
        " \n",
        "def ar_FRE(txt):\n",
        "    sentence = sentenceCount(txt)\n",
        "    word = wordCount(txt)\n",
        "    syllable = ar_syllables_count(txt)\n",
        "    ASL = float(word) / sentence\n",
        "    ASW = float(syllable) / word\n",
        "    RE = 206.835 - (1.015 * ASL) - (84.6 * ASW)\n",
        "    return round(RE,1)\n",
        " \n",
        "def ar_NDC(txt):\n",
        "    c_word = ar_count_complex_word(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence = sentenceCount(txt)\n",
        "    PDW = round(float(c_word) / word,2)*100\n",
        "    ASL = float(word) / sentence\n",
        "    if PDW > 5:\n",
        "        RS = ((0.1579 * PDW) + (0.0496 * ASL)) + 3.6365\n",
        "    else:\n",
        "        RS = (0.1579 * PDW) + (0.0496 * ASL)\n",
        "    return round(RS,1)\n",
        " \n",
        "def ar_SPACHE(txt):\n",
        "    c_word = ar_count_complex_word(txt)\n",
        "    word = wordCount(txt)\n",
        "    sentence = sentenceCount(txt)\n",
        "    PDW = round(float(c_word) / word,2)*100\n",
        "    ASL = float(word) / sentence\n",
        "    SP = ((0.141 *ASL) + (0.086 * PDW)+0.839)\n",
        "    return round(SP,1)  \n",
        " \n",
        "#Sentences > 30 Syllables\n",
        "def ar_s_g_30s(txt):\n",
        "    count =0\n",
        "    sentence=sent_tokenize(txt)\n",
        "    for x in sentence:\n",
        "        #print(syllableCount(x))\n",
        "        if ar_syllables_count(x) > 30:\n",
        "            count+=1\n",
        "    return count    \n",
        " \n",
        "#Sentences > 20 Syllables\n",
        "def ar_s_g_20s(txt):\n",
        "    count =0\n",
        "    sentence=sent_tokenize(txt)\n",
        "    for x in sentence:\n",
        "        #print(syllableCount(x))\n",
        "        if ar_syllables_count(x) > 20:\n",
        "            count+=1\n",
        "    return count    \n",
        " \n",
        "#Words > 4 Syllables\n",
        "def ar_w_g_4s(txt):\n",
        "    count =0\n",
        "    words=word_tokenize(txt)\n",
        "    for x in words:\n",
        "        #print(syllableCount(x))\n",
        "        if ar_syllables(x) >= 4:\n",
        "            count+=1\n",
        "    return count  \n",
        " \n",
        " \n",
        "\n",
        " \n",
        "#Syllables Per Word\n",
        "def ar_SPW(txt):\n",
        "  WordCount=wordCount(txt)\n",
        "  SyllableCount= ar_syllables_count(txt)\n",
        "  SyllablesPerWord=round(float(SyllableCount)/WordCount,1)\n",
        "  return SyllablesPerWord\n",
        " \n",
        "def ar_spellingIssues(text):\n",
        "   res = SpCH(text)\n",
        "   count = 0\n",
        "   for i in res:\n",
        "     if i == '/':\n",
        "       count +=1\n",
        "   return count    \n",
        " \n",
        "def Al_Heeti(text):\n",
        "   words = wordCount(text)\n",
        "   chars = characterCount(text)\n",
        "   F3 = float(chars) / words\n",
        "   Al = (F3 * 4.414 ) - 13.468\n",
        "   return round(Al,2)\n",
        "\n",
        "def ARI(text):\n",
        "  words = wordCount(text)\n",
        "  chars = characterCount(text)\n",
        "  sentences = sentenceCount(text)\n",
        "  F3 = float(chars) / words\n",
        "  F5 = float(words) / sentences\n",
        "  ARI = (F3*4.71) + (F5 * 0.5) - 21.43\n",
        "  return round(ARI,2) \n",
        "\n",
        "def AARI(text):\n",
        "  words = wordCount(text)\n",
        "  chars = characterCount(text)\n",
        "  sentences = sentenceCount(text)\n",
        "  F3 = float(chars) / words\n",
        "  F5 = float(words) / sentences\n",
        "  AARI = ((chars*3.28)+(F3 * 1.43)+(F5 * 1.24)+472.42)/1046.3\n",
        "  return round(AARI,2)\n",
        "\n",
        "def ar_ReadingTime(text):\n",
        "    word = wordCount(text)\n",
        "    div_wordCount=float(word)/110\n",
        "    second,minute=math.modf(div_wordCount)\n",
        "    if second >= 0.60:\n",
        "      minute +=1\n",
        "      second-=0.60\n",
        "    #second=round((second*0.60),2)\n",
        "    #print(\"Reading Time is\",minute+second);\n",
        "    return int(minute),int(round(second,2)*100)\n",
        "\n",
        "def ar_SpeakingTime(text):\n",
        "    word = wordCount(text)\n",
        "    speakingRate=84\n",
        "    time=word/speakingRate\n",
        "    second,minute=math.modf(time)\n",
        "    if second >= 0.60:\n",
        "      minute +=1\n",
        "      second-=0.60\n",
        "    #second=round((second*0.60),2)\n",
        "    #print(\"Reading Time is\",minute+second);\n",
        "    return int(minute),int(round(second,2)*100)\n",
        "\n",
        "def ar_results(t):\n",
        "  print(\"############################################################Text Statistics############################################################\")\n",
        "  print(\"Character Count : {}\".format(characterCount(t)))\n",
        "  print(\"Syllable Count : {}\".format(ar_syllables_count(t)))\n",
        "  print(\"Word Count : {}\".format(wordCount(t)))\n",
        "  print(\"Unique Word Count : {}\".format(uniqueWordCount(t)))\n",
        "  print(\"Sentence Count : {}\".format(sentenceCount(t)))\n",
        "  print(\"Paragraph Count : {}\".format(paragraphCount(t)))\n",
        "  print(\"Reading Time: {}\".format(ar_ReadingTime(t)))\n",
        "  print(\"Speaking Time: {}\".format(ar_SpeakingTime(t)))\n",
        "  print(\"############################################################Farasa APIs############################################################\")\n",
        "  print(\"Segmentaion: {}\".format(Seg(t)))\n",
        "  print(\"Lemmatization: {}\".format(lemma(t)))\n",
        "  print(\"POS: {}\".format(POS(t)))\n",
        "  print(\"SpellChecker: {}\".format(SpCH(t)))\n",
        "  print(\"Diacritization: {}\".format(Diac(t)))\n",
        "  print(\"Diacritizationv2: {}\".format(Diac2(t)))\n",
        "  print(\"############################################################Arabic Measures############################################################\")\n",
        "  print(\"Al-Heeti Formula: {}\".format(Al_Heeti(t)))\n",
        "  print(\"ARI Formula: {}\".format(ARI(t)))\n",
        "  print(\"AARI Formula: {}\".format(AARI(t)))\n",
        "  print(\"############################################################Readability Grade Levels############################################################\")\n",
        "  print(\"Flesch-Kincaid Grade Level : {}\".format(ar_FKRA(t)))\n",
        "  print(\"Gunning Fog Index : {}\".format(ar_GFI(t)))\n",
        "  print(\"Coleman–Liau index : {}\".format(ar_CLI(t)))\n",
        "  print(\"SMOG Index : {}\".format(ar_SMOGI(t)))\n",
        "  print(\"Automated Readability Index : {}\".format(ar_ARI(t)))\n",
        "  print(\"FORCAST Grade Level : {}\".format(ar_FORCAST(t)))\n",
        "  print(\"Powers Sumner Kearl Grade : {}\".format(ar_PSKG(t)))\n",
        "  print(\"RIX Readability : {}\".format(ar_RIX(t)))\n",
        "  print(\"############################################################Readability Scores############################################################\")\n",
        "  print(\"Flesch Reading Ease : {}\".format(ar_FRE(t)))\n",
        "  print(\"Spache Score : {}\".format(ar_SPACHE(t)))\n",
        "  print(\"New Dale-Chall Score : {}\".format(ar_NDC(t)))\n",
        "  print(\"Lix Readability: {}\".format(LIX(t)))\n",
        "  print(\"Lensear Write: {}\".format(LensearWrite(t)))\n",
        "  print(\"############################################################Readability Issues############################################################\")\n",
        "  print(\"Sentences > 30 Syllables = {}\\t{}%\".format(ar_s_g_30s(t),int((ar_s_g_30s(t)/sentenceCount(t)*100))))\n",
        "  print(\"Sentences > 20 Syllables = {}\\t{}%\".format(ar_s_g_20s(t),int((ar_s_g_20s(t)/sentenceCount(t)*100))))\n",
        "  print(\"Words > 4 Syllables= {}\\t{}%\".format(ar_w_g_4s(t),int((ar_w_g_4s(t)/wordCount(t)*100))))\n",
        "  print(\"Words > 12 Letters= {}\\t{}%\".format(w_g_12l(t),int((w_g_12l(t)/wordCount(t)*100))))\n",
        "  print(\"############################################################Text Density Issues############################################################\")\n",
        "  print(\"Characters Per Word : {}\".format(CPW(t)))\n",
        "  print(\"Syllables Per Word : {}\".format(ar_SPW(t)))\n",
        "  print(\"Words Per Sentence : {}\".format(WPS(t)))\n",
        "  print(\"Words Per Paragraph : {}\".format(WPP(t)))\n",
        "  print(\"Sentences Per Paragraph : {}\".format(SPP(t)))\n",
        "  print(\"Spelling Issues: {}\".format(ar_spellingIssues(t)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPGrkb0fIrxL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3e91006-5fc0-459c-ea28-63d5df6ee21d"
      },
      "source": [
        "t =\"\"\"\n",
        "هَلْ ذَهَبْتَ إِلَى حَدِيقَةِ الْحَيَوَانِ؟ نَعَمْ، ذَهَبْتُ إَلَى حَدِيقَةِ الْحَيَوَانِ. لَا، لَمْ أَذْهَبْ إِلَى حَدِيقَةِ الْحَيَوَانِ.\n",
        "مَنْ قَدَّمَ إِلَيْكَ الْهَدِيَّةَ؟ أَحْمَدُ صَدِيقِي هُوَ الَّذِي قَدَّمَ إِلَى الْهَدِيَّةَ.\n",
        "مَا أَحَبُّ الْمَوَادِّ الدِّرَاسِيَّةِ إِلَيْكَ ؟ اللُّغَةُ الْعَرَبِيَّةُ أَحَبُّ الْمَوَادِّ إِلَيَّ.\n",
        "مَتَى تَذْهَبُ إِلَى الشَّاطِئ؟ أَذْهَبُ إِلَى الشَّاطِئ فِي فَصْلِ الصَّيفِ.\n",
        "أَيْنَ يُقَامُ الْحَفْلُ؟ يُقَامُ الْحَفْلُ فِي مَسْرَحِ الْمَدْرَسَةِ.\n",
        "\"\"\"\n",
        "ar_results(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############################################################Text Statistics############################################################\n",
            "Character Count : 228\n",
            "Syllable Count : 163\n",
            "Word Count : 55\n",
            "Unique Word Count : 38\n",
            "Sentence Count : 11\n",
            "Paragraph Count : 5\n",
            "Reading Time: (0, 50)\n",
            "Speaking Time: (1, 5)\n",
            "############################################################Farasa APIs############################################################\n",
            "Segmentaion: ['هل', 'ذهب+ت', 'إلى', 'حديق+ة', 'ال+حيوان', '؟', 'نعم', '،', 'ذهب+ت', 'إلى', 'حديق+ة', 'ال+حيوان', '.', 'لا', '،', 'لم', 'أذهب', 'إلى', 'حديق+ة', 'ال+حيوان', '.', 'من', 'قدم', 'إلي+ك', 'ال+هدي+ة', '؟', 'أحمد', 'صديق+ي', 'هو', 'الذي', 'قدم', 'إلى', 'ال+هدي+ة', '.', 'ما', 'أحب', 'ال+مواد', 'ال+دراسي+ة', 'إلي+ك', '؟', 'ال+لغ+ة', 'ال+عربي+ة', 'أحب', 'ال+مواد', 'إلي', '.', 'متى', 'تذهب', 'إلى', 'ال+شاطئ', '؟', 'أذهب', 'إلى', 'ال+شاطئ', 'في', 'فصل', 'ال+صيف', '.', 'أين', 'يقام', 'ال+حفل', '؟', 'يقام', 'ال+حفل', 'في', 'مسرح', 'ال+مدرس+ة', '.']\n",
            "Lemmatization: ['هل', 'ذهب', 'إلى', 'حديقة', 'حيوان', '؟', 'نعم', '،', 'ذهب', 'إلى', 'حديقة', 'حيوان', '.', 'لا', '،', 'لم', 'أذهب', 'إلى', 'حديقة', 'حيوان', '.', 'من', 'قدم', 'إلى', 'هدية', '؟', 'أحمد', 'صديق', 'هو', 'الذي', 'قدم', 'إلى', 'هدية', '.', 'ما', 'أحب', 'مادة', 'دراسي', 'إلى', '؟', 'لغة', 'عربي', 'أحب', 'مادة', 'إلى', '.', 'متى', 'ذهب', 'إلى', 'شاطئ', '؟', 'أذهب', 'إلى', 'شاطئ', 'في', 'فصل', 'صيف', '.', 'أين', 'أقام', 'حفل', '؟', 'أقام', 'حفل', 'في', 'مسرح', 'مدرسة', '.']\n",
            "PART  : 8\n",
            "V+PRON  : 2\n",
            "PREP  : 12\n",
            "NOUN+NSUFF  : 3\n",
            "DET+NOUN  : 10\n",
            "PUNC  : 13\n",
            "V  : 8\n",
            "NOUN  : 5\n",
            "PRON  : 4\n",
            "DET+NOUN+NSUFF  : 4\n",
            "DET+ADJ+NSUFF  : 2\n",
            "POS: None\n",
            "SpellChecker: هل ذهبت إلى حديقة الحيوان ؟ نعم ، ذهبت إلى حديقة الحيوان . لا ، لم أذهب إلى حديقة الحيوان .\n",
            "من قدم إليك الهدية ؟ أحمد صديقي هو الذي قدم إلى الهدية .\n",
            "ما أحب المواد الدراسية إليك ؟ اللغة العربية أحب المواد إلى/إلي .\n",
            "متى تذهب إلى الشاطئ ؟ أذهب إلى الشاطئ في فصل الصيف .\n",
            "أين يقام الحفل ؟ يقام الحفل في مسرح المدرسة .\n",
            "\n",
            "Diacritization: هَلْ ذَهَبتْ إِلَى حَديقَةِ الحَيَوانِ ؟ نَعَمْ ، ذَهَبتْ إِلَى حَديقَةِ الحَيَوانِ . لا ، لَمْ أَذْهَبْ إِلَى حَديقَةِ الحَيَوانِ . مِنْ قَدَّمَ إِلَيْكَ الهَديَّةُ ؟ أَحْمَدُ صَديقي هوَ اَلَّذي قَدِمَ إِلَى الهَديَّةِ . ما أُحِبُّ المَوادَّ الدِّراسيَّةَ إِلَيْكَ ؟ اللُّغَةُ العَرَبيَّةُ أَحَبُّ المَوادَّ إِلَيّْ . مَتَى تَذْهَبُ إِلَى الشّاطِئِ ؟ أَذْهَبُ إِلَى الشّاطِئِ في فَصْلِ الصَّيْفِ . أَيْنَ يُقامُ الحَفْلَ ؟ يُقامُ الحَفْلُ في مَسْرَحِ المَدْرَسَةِ .\n",
            "Diacritizationv2: هَلْ ذَهَبْتَ إِلَى حَدِيقَةِ الْحَيَوَانِ ؟ نَعَمْ ، ذَهَبُتْ إِلَى حَدِيقَةِ الْحَيَوَانِ . لَا ، لَمْ أَذْهَبْ إِلَى حَدِيقَةِ الْحَيَوَانِ .\n",
            "مِنْ قَدَّمَ إِلَيْكَ الْهَدِيَّةَ ؟ أَحْمَدُ صَدِيقِي هُوَ اَلَّذِي قَدِمَ إِلَى الْهَدِيَّةِ .\n",
            "مَا أُحِبُّ الْمَوَادَّ الدِّرَاسِيَّةَ إِلَيْكَ ؟ اللُّغَةُ الْعَرَبِيَّةُ أَحَبُّ الْمَوَادَّ إِلَيَّ .\n",
            "مَتَى تَذْهَبُ إِلَى الشَّاطِئِ ؟ أَذْهَبْ إِلَى الشَّاطِئِ فِي فَصْلِ الصَّيْفِ .\n",
            "أَيْنَ يُقَامُ الْحَفْلُ ؟ يُقَامُ الْحَفْلُ فِي مَسْرَحِ الْمَدْرَسَةِ .\n",
            "############################################################Arabic Measures############################################################\n",
            "Al-Heeti Formula: 4.83\n",
            "ARI Formula: 0.6\n",
            "AARI Formula: 1.18\n",
            "############################################################Readability Grade Levels############################################################\n",
            "Flesch-Kincaid Grade Level : 21.33\n",
            "Gunning Fog Index : 16.55\n",
            "Coleman–Liau index : 2.66\n",
            "SMOG Index : 10.83\n",
            "Automated Readability Index : 0.6\n",
            "FORCAST Grade Level : 18.09\n",
            "Powers Sumner Kearl Grade : 11.67\n",
            "RIX Readability : 1.91\n",
            "############################################################Readability Scores############################################################\n",
            "Flesch Reading Ease : -49.0\n",
            "Spache Score : 4.6\n",
            "New Dale-Chall Score : 9.6\n",
            "Lix Readability: 74.0\n",
            "Lensear Write: 50.0\n",
            "############################################################Readability Issues############################################################\n",
            "Sentences > 30 Syllables = 2\t18%\n",
            "Sentences > 20 Syllables = 4\t36%\n",
            "Words > 4 Syllables= 19\t34%\n",
            "Words > 12 Letters= 5\t9%\n",
            "############################################################Text Density Issues############################################################\n",
            "Characters Per Word : 4.1\n",
            "Syllables Per Word : 3.0\n",
            "Words Per Sentence : 5.0\n",
            "Words Per Paragraph : 11.0\n",
            "Sentences Per Paragraph : 2.2\n",
            "Spelling Issues: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F10vdxHR60U2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "outputId": "aa060580-f705-4274-b113-f20d9e718c74"
      },
      "source": [
        "from indicnlp.tokenize import sentence_tokenize\n",
        "from indicnlp.tokenize import indic_tokenize \n",
        "import syllables\n",
        "import math\n",
        "def removePunctuation(text):\n",
        "    result = \"\"\n",
        " \n",
        "    for char in text:\n",
        "        if char in (\".\",\",\",\"!\",\"?\",\"؟\",\"،\",\"\\\",\"\"/\",\"\\\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\":\",\";\",\"<\",\">\",\"=\",\"[\",\"]\",\"^\",\"_\",\"`\",\"{\",\"}\",\"|\",\"~\"):\n",
        "            continue\n",
        "        if char in (\"-\",\"\\n\",\"\\r\",\"\\t\"):\n",
        "            char=\" \"\n",
        "        result+=char\n",
        "    return result\n",
        "\n",
        "def characterCountHindi(text):\n",
        "    #sentences=sentence_tokenize.sentence_split(text, lang='hi')\n",
        "    count=0\n",
        "    for t in indic_tokenize.trivial_tokenize(text):\n",
        "        for i in t:\n",
        "            count=count+1\n",
        "    return count\n",
        "    \n",
        "def sentenceCountHindi(text):\n",
        "    #text=removePunctuation(text)\n",
        "    sentences=sentence_tokenize.sentence_split(text, lang='hi')\n",
        "    return (len(sentences))\n",
        "\n",
        "def wordCountHindi(text):\n",
        "    text=removePunctuation(text)\n",
        "    text=text.split()\n",
        "    return(len(text))\n",
        "\n",
        "def uniquewordCountHindi(text):\n",
        "    count=0\n",
        "    text=removePunctuation(text)\n",
        "    words=text.split()\n",
        "    uniqueWords=[]\n",
        "    for i in words:\n",
        "        if i not in uniqueWords:\n",
        "            #print(i)\n",
        "            uniqueWords.append(i)\n",
        "    for word in uniqueWords:\n",
        "        if word.isdigit():\n",
        "            continue;\n",
        "        count+=1\n",
        "    return count\n",
        "\n",
        "def syllableCountHindi(text):\n",
        "    count=0\n",
        "    text=removePunctuation(text)\n",
        "    words=text.split()\n",
        "    for i in words:\n",
        "        count+=syllables.estimate(i)\n",
        "    return count\n",
        "\n",
        "def polysyllabicHindi(text):\n",
        "    count=0\n",
        "    num_syl=0\n",
        "    text=removePunctuation(text)\n",
        "    words=text.split()\n",
        "    for i in words:\n",
        "        num_syl=syllables.estimate(i)\n",
        "        if(num_syl>=2):\n",
        "            count+=1\n",
        "    return count\n",
        "\n",
        "def longWordCountHindi(text):\n",
        "    words=indic_tokenize.trivial_tokenize(text)\n",
        "    count=0\n",
        "    for i in words:\n",
        "        if len(i)>6:\n",
        "            count+=1\n",
        "    return count\n",
        "def singleSyllableWordHindi(text):\n",
        "    count=0\n",
        "    num_syl=0\n",
        "    text=removePunctuation(text)\n",
        "    words=text.split()\n",
        "    for i in words:\n",
        "        num_syl=syllables.estimate(i)\n",
        "        if(num_syl==1):\n",
        "            count+=1\n",
        "    return count\n",
        "    \n",
        "\n",
        "def paragraphCount(text):\n",
        "    count=  [line.strip() for line in text.split('\\n') if len(line.strip()) > 0]\n",
        "    #print(\"Paragraph Count is\",len(count))\n",
        "    return len(count)\n",
        "\n",
        "def FKRA(text):\n",
        "    #txt = txt.lower()\n",
        "    #txt=removePunctuation(txt)\n",
        "    word = wordCountHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    syllable =syllableCountHindi(text)\n",
        "    ASL = float(word)/sentence\n",
        "    ASW = float(syllable)/word\n",
        "    FKRA = (0.39 * ASL) + (11.8 * ASW) - 15.59\n",
        "    return round(FKRA,2)\n",
        "\n",
        "def GFI(text):\n",
        "    word = wordCountHindi(text)\n",
        "    PSW=polysyllabicHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    ASL = float(word)/sentence\n",
        "    #syllable =syllableCount(txt)\n",
        " \n",
        "    GFI = 0.4 * ( (ASL) + (PSW))\n",
        "    return round(GFI,2)\n",
        "\n",
        "def CLI(text):\n",
        "    char = characterCountHindi(text)\n",
        "    word = wordCountHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    L = (float(char) / word) * 100\n",
        "    S = (float(sentence) / word) * 100\n",
        "    CLI = (0.0588 * L) - (0.296 * S) - 15.8\n",
        "    return round(CLI,2)\n",
        "\n",
        "def SMOGI(text):\n",
        "    PSW=polysyllabicHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    SMOGI = 1.0430 * (math.sqrt(PSW * (30 / float(sentence)))) + 3.1291\n",
        "    return round(SMOGI,2)\n",
        " \n",
        "\n",
        "def ARI(text):\n",
        "    char = characterCountHindi(text)\n",
        "    word = wordCountHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    ARI = (4.71 * (float(char) / word)) + (.5 * (float(word) / sentence)- 21.43)\n",
        "    return round(ARI , 2)\n",
        " \n",
        "def FORCAST(text):\n",
        "    syll=singleSyllableWordHindi(text)\n",
        "    c_word = polysyllabicHindi(text)\n",
        "    word = wordCountHindi(text)\n",
        "    div=round(float(word*10)/150,2)\n",
        "    #print(div)\n",
        "    GL = 20 - (float(syll)/ div)\n",
        "    return round(GL,2)\n",
        " \n",
        "def PSKG(text):\n",
        "    #txt=removePunctuation(txt)\n",
        "    word = wordCountHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    syllable =syllableCountHindi(text)\n",
        "    ASL = float(word) / sentence\n",
        "    #NS = (syllable * 0.0455) \n",
        "    #GL = (0.0778 * ASL) + NS + 2.7971\n",
        "    NS = ((float(syllable)/word) * 0.0455*100) \n",
        "    GL = (0.0778 * ASL) + NS - 2.2029\n",
        "    return round(GL,2)\n",
        " \n",
        "def RIX(text):\n",
        "    count = 0\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    for x in text.split(\" \"):\n",
        "        #print(characterCount(x))\n",
        "        if characterCountHindi(text) > 4:\n",
        "            count+=1\n",
        "    return float(count) / sentence     \n",
        "    \n",
        "\n",
        "\n",
        "def FRE(txt):\n",
        "    word = wordCountHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    syllable =syllableCountHindi(text)\n",
        "    ASL = float(word) / sentence\n",
        "    ASW = float(syllable) / word\n",
        "    RE=(0.39*ASL)+(11.8*ASW)-15.59\n",
        "    #RE = 206.835 - (1.015 * ASL) - (84.6 * ASW)\n",
        "    return round(RE,1)\n",
        "\n",
        "def NDC(text):\n",
        "    c_word = polysyllabicHindi(text)\n",
        "    word = wordCountHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    PDW = round(float(c_word) / word,2)*100\n",
        "    ASL = float(word) / sentence\n",
        "    if PDW > 5:\n",
        "        RS = ((0.1579 * PDW) + (0.0496 * ASL)) + 3.6365\n",
        "    else:\n",
        "        RS = (0.1579 * PDW) + (0.0496 * ASL)\n",
        "    return round(RS,1)\n",
        " \n",
        "def SPACHE(text):\n",
        "    c_word = polysyllabicHindi(text)\n",
        "    word = wordCountHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    PDW = round(float(c_word) / word,2)*100\n",
        "    ASL = float(word) / sentence\n",
        "    SP = ((0.141 *ASL) + (0.086 * PDW)+0.839)\n",
        "    return round(SP,1)\n",
        "\n",
        "def LIX(text):\n",
        "    word = wordCountHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    LongWords=longWordCountHindi(text)\n",
        "    percentageOfLongWords=(float(LongWords)/word)*100\n",
        "    avgLengthOfSentence=float(word)/sentence\n",
        "    result=round(percentageOfLongWords+avgLengthOfSentence,0)\n",
        "    return result\n",
        " \n",
        "\n",
        "def LensearWrite(text):\n",
        "    text=removePunctuation(text)\n",
        "    words=text.split()\n",
        "    word = wordCountHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    #hardword=3\n",
        "    #easyword=1\n",
        "    ratio=100/word\n",
        "    hardword=ratio*3\n",
        "    easyword=ratio*1\n",
        "    Score=0\n",
        "    for w in words:\n",
        "        if syllables.estimate(w) <= 2:\n",
        "            Score+=easyword\n",
        "        if syllables.estimate(w) >=3:\n",
        "            Score+=hardword\n",
        "    preResult=(Score/sentence)\n",
        "    if preResult > 20:\n",
        "        result=round((float(preResult)/2),1)\n",
        "    else:\n",
        "        result=round(float(preResult-2)/2,1)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def s_g_30Hindi(text):\n",
        "    count=0\n",
        "    num_syl=0\n",
        "    sentences=sentence_tokenize.sentence_split(text, lang='hi')\n",
        "    for i in sentences:\n",
        "        if(syllables.estimate(i)>30):\n",
        "            count+=1\n",
        "    return count\n",
        " \n",
        "#Sentences > 20 Syllables\n",
        "def s_g_20Hindi(text):\n",
        "    count=0\n",
        "    num_syl=0\n",
        "    sentences=sentence_tokenize.sentence_split(text, lang='hi')\n",
        "    for i in sentences:\n",
        "        if(syllables.estimate(i)>20):\n",
        "            count+=1\n",
        "    return count   \n",
        " \n",
        "#Words > 4 Syllables\n",
        "def w_g_4Hindi(text):\n",
        "    count =0\n",
        "    words=text.split()\n",
        "    for x in words:\n",
        "        if(syllables.estimate(x)>4):\n",
        "            count+=1\n",
        "    return count \n",
        " \n",
        "#Words > 12 Letters\n",
        "def w_g_12Hindi(text):\n",
        "    count =0\n",
        "    words=text.split()\n",
        "    for x in words:\n",
        "        #print(len(x))\n",
        "        if len(x) > 12:\n",
        "            count+=1\n",
        "    return count \n",
        " \n",
        "# Characters Per Word\n",
        "def CPWHindi(text):\n",
        "    word = wordCountHindi(text)\n",
        "    char=characterCountHindi(text)\n",
        "    CharactersPerWord=round(float(char)/word,1)\n",
        "    return CharactersPerWord\n",
        " \n",
        "#Syllables Per Word\n",
        "def SPWHindi(text):\n",
        "    word = wordCountHindi(text)\n",
        "    syllable =syllableCountHindi(text)\n",
        "    SyllablesPerWord=round(float(syllable)/word,1)\n",
        "    return SyllablesPerWord\n",
        " \n",
        "#Words Per Sentence\n",
        "def WPSHindi(text):\n",
        "    word = wordCountHindi(text)\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    WordsPerSentence=round(float(word)/sentence,1)\n",
        "    return WordsPerSentence\n",
        " \n",
        "#Words Per Paragraph\n",
        "def WPPHindi(txt):\n",
        "    word = wordCountHindi(text)\n",
        "    paragraph= paragraphCount(text)\n",
        "    WordsPerParagraph=round(float(word)/paragraph,1)\n",
        "    return WordsPerParagraph\n",
        " \n",
        "#Sentences Per Paragraph\n",
        "def SPPHindi(txt):\n",
        "    sentence =sentenceCountHindi(text)\n",
        "    paragraph= paragraphCount(text)\n",
        "    SentencePerParagraph=round(float(sentence)/paragraph,1)\n",
        "    return SentencePerParagraph\n",
        "\"\"\"\n",
        "def w_g_6l(txt):\n",
        "    count =0\n",
        "    words=word_tokenize(txt)\n",
        "    for x in words:\n",
        "        #print(len(x))\n",
        "        if len(x) > 6:\n",
        "            count+=1\n",
        "    return count \n",
        "\"\"\"\n",
        "\n",
        "def spellCheck(text):\n",
        "    count=0\n",
        "    text=removePunctuation(text)\n",
        "    words=word_tokenize(text)\n",
        "    #print(words)\n",
        "    for i in words:\n",
        "        if(i.isdigit()):\n",
        "            continue;\n",
        "        spellcheck = TextBlob(i)\n",
        "        b=str(spellcheck.correct())\n",
        "        print(b)\n",
        "        \n",
        "    #print(\"Word Count is \",count)\n",
        "    return count\n",
        "\n",
        "text1=\"Hello mi nam is Rashi\"\n",
        "spellCheck(text1)\n",
        "text=\"लोनपो गार तिब्बत के बत्तीसवें राजा सौनगवसेन गांपो के मंत्री थे। वे\"\n",
        "print(\"Character Count : \",characterCountHindi(text))\n",
        "print(\"Syllable Count: \",syllableCountHindi(text))\n",
        "print(\"Word Count: \", wordCountHindi(text))\n",
        "print(\"Unique Word Count: \",uniquewordCountHindi(text))\n",
        "print(\"Sentence Count: \",sentenceCountHindi(text))\n",
        "print(\"Paragraph Count: \",paragraphCount(text))\n",
        "\n",
        "print(\"Flesch-Kincaid Grade Level : \", FKRA(text))\n",
        "print(\"Gunning Fog Index : \",GFI(text))\n",
        "print(\"Coleman–Liau index : \",CLI(text))\n",
        "print(\"SMOG Index : \",SMOGI(text))\n",
        "print(\"Automated Readability Index : \",ARI(text))\n",
        "print(\"FORCAST Grade Level : \",FORCAST(text))\n",
        "print(\"Powers Sumner Kearl Grade : \",PSKG(text))\n",
        "print(\"RIX Readability : \",RIX(text))\n",
        "print(\"Flesch Reading Ease : \",FRE(text))\n",
        "print(\"Spache Score : \",SPACHE(text))\n",
        "print(\"New Dale-Chall Score : \",NDC(text))\n",
        "print(\"Lix Readability: \",LIX(text))\n",
        "print(\"Lensear Write: \",LensearWrite(text))\n",
        "\n",
        "print(\"Sentences > 30 Syllables = {}\\t{}%\".format(s_g_30Hindi(text),int((s_g_30Hindi(text)/sentenceCountHindi(text)*100))))\n",
        "print(\"Sentences > 20 Syllables = {}\\t{}%\".format(s_g_20Hindi(text),int((s_g_20Hindi(text)/sentenceCountHindi(text)*100))))\n",
        "print(\"Words > 4 Syllables= {}\\t{}%\".format(w_g_4Hindi(text),int((w_g_4Hindi(text)/wordCountHindi(text)*100))))\n",
        "print(\"Words > 12 Letters= {}\\t{}%\".format(w_g_12Hindi(text),int((w_g_12Hindi(text)/wordCountHindi(text)*100))))\n",
        "\n",
        "print(\"Characters Per Word : \",CPWHindi(text))\n",
        "print(\"Syllables Per Word : \",SPWHindi(text))\n",
        "print(\"Words Per Sentence : \",WPSHindi(text))\n",
        "print(\"Words Per Paragraph : \",WPPHindi(text))\n",
        "print(\"Sentences Per Paragraph : \",SPPHindi(text))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello\n",
            "mi\n",
            "am\n",
            "is\n",
            "Wash\n",
            "Character Count :  55\n",
            "Syllable Count:  12\n",
            "Word Count:  12\n",
            "Unique Word Count:  11\n",
            "Sentence Count:  2\n",
            "Paragraph Count:  1\n",
            "Flesch-Kincaid Grade Level :  -1.45\n",
            "Gunning Fog Index :  2.4\n",
            "Coleman–Liau index :  6.22\n",
            "SMOG Index :  3.13\n",
            "Automated Readability Index :  3.16\n",
            "FORCAST Grade Level :  5.0\n",
            "Powers Sumner Kearl Grade :  2.81\n",
            "RIX Readability :  6.0\n",
            "Flesch Reading Ease :  -1.4\n",
            "Spache Score :  1.7\n",
            "New Dale-Chall Score :  0.3\n",
            "Lix Readability:  23.0\n",
            "Lensear Write:  25.0\n",
            "Sentences > 30 Syllables = 0\t0%\n",
            "Sentences > 20 Syllables = 0\t0%\n",
            "Words > 4 Syllables= 0\t0%\n",
            "Words > 12 Letters= 0\t0%\n",
            "Characters Per Word :  4.6\n",
            "Syllables Per Word :  1.0\n",
            "Words Per Sentence :  6.0\n",
            "Words Per Paragraph :  12.0\n",
            "Sentences Per Paragraph :  2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfKa7QKclZQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ta=\"\"\"\n",
        "The biodiversity and the biodensity in the ocean is higher, in places, than it is in the rainforests. It's mostly unexplored, and yet there are beautiful sights like this that captivate us and make us become familiar with it.\n",
        "But when you're standing at the beach, I want you to think that you're standing at the edge of a very unfamiliar world.\n",
        "We have to have a very special technology to get into that unfamiliar world.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmK_PQpiQh9F",
        "colab_type": "text"
      },
      "source": [
        "# Detect language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rsAbl1kQqGh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "34c9bd07-315f-4761-f7c3-d4d94b9844f2"
      },
      "source": [
        "print(\"Enter your input here:\")\n",
        "X =input()\n",
        "language = detect(X)\n",
        "if language == 'en':\n",
        "  print(X)\n",
        "  print(\"***********************************************************\")\n",
        "  print(\"Language Detect is English\")\n",
        "  TextComposition=process_content(X)\n",
        "  en_results(X)\n",
        "\n",
        "elif language == 'ar':\n",
        "  print(X)\n",
        "  print(\"***********************************************************\")\n",
        "  print(\"Language Detect is Arabic\")\n",
        "  ar_results(X)\n",
        "  # بِاسْتِخْدَامِ الشَّبَكَةِ الْمَعْلُومَاتِيَّةِ اجْمَعْ بَعْضَ الصُّوَرِ الْمُعَبِّرَةِ عَنِ الْأَمْرِ بِالْمَعْرُوفِ وَالنَّهْيِ عَنِ الْمُنْكَرِ فِي مُجْتَمَعِكَ"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter your input here:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-03ea738a25cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your input here:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'en'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNRESuhSSPsq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL93Olxd4fRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "ea7b5a76-18b9-4f2f-d504-4565972b6efa"
      },
      "source": [
        " \n",
        "def allOutput(t):\n",
        "  print(characterCount(t),\"\\t\",syllableCount(t),\"\\t\",wordCount(t),\"\\t\",uniqueWordCount(t),\"\\t\",sentenceCount(t),\"\\t\",paragraphCount(t),\"\\t\",ReadingTime(t),\"\\t\",SpeakingTime(t),\"\\t\",getAdjectivesCount(),\"\\t\",getAdverbsCount(),\"\\t\",getConjuctionsCount(),\"\\t\",getDeterminersCount(),\"\\t\",getInterjectionsCount(),\"\\t\",getNounsCount(),\"\\t\",getPropernounCount(),\"\\t\",getPrepositionsCount(),\"\\t\",getPronounsCount(),\"\\t-\",\"\\t\",getVerbsCount(),\"\\t-\",\"\\t\",nonWordCount(t),\"\\t-\",\"\\t\",FKRA(t),\"\\t\",\n",
        "  GFI(t),\"\\t\",CLI(t),\"\\t\",SMOGI(t),\"\\t\",ARI(t),\"\\t\",FORCAST(t),\"\\t\",PSKG(t),\"\\t\",RIX(t),\"\\t-\",\"\\t-\",\"\\t\",FRE(t),\"\\t-\\t-\\t\",SPACHE(t),\"\\t\",NDC(t),\"\\t\",LIX(t),\"\\t\",LensearWrite(t),\"\\t\",spellingIssues(t),\"\\t\",grammarIssues(t),\"\\t\",s_g_30s(t),\"\\t\",s_g_20s(t),\"\\t\",\n",
        "  w_g_4s(t),\"\\t\",w_g_12l(t),\"\\t\",passiveCount(t),\"\\t\",getAdverbsCount(),\"\\t-\",\"\\t\",CPW(t),\"\\t\",SPW(t),\"\\t\",WPS(t),\"\\t\",WPP(t),\"\\t\",SPP(t))\n",
        "t=\"\"\"\n",
        "The biodiversity and the biodensity in the ocean is higher, in places, than it is in the rainforests. It's mostly unexplored, and yet there are beautiful sights like this that captivate us and make us become familiar with it.\n",
        "But when you're standing at the beach, I want you to think that you're standing at the edge of a very unfamiliar world.\n",
        "We have to have a very special technology to get into that unfamiliar world.\n",
        "\"\"\"\n",
        "TextComposition=process_content(t)\n",
        "allOutput(t)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Answer- 11\n",
            "Non word:  []\n",
            "336 \t 114 \t 76 \t 52 \t 4 \t 3 \t 0.23 \t 0.36 \t 7 \t 4 \t 4 \t 11 \t 0 \t 11 \t 0 \t 11 \t 10 \t- \t 16 \t- \t 0 \t- \t 9.52 \t 12.34 \t 8.64 \t 11.7 \t 8.89 \t 9.15 \t 6.1 \t 6.75 \t- \t- \t 60.7 \t-\t-\t 4.5 \t 6.5 \t 36.0 \t 63.2 \t 6 \t 1 \t 2 \t 4 \t 5 \t 0 \t 0 \t 4 \t- \t 4.4 \t 1.5 \t 19.0 \t 25.3 \t 1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WpZhYI_SP7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XQYWt4kSQGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IcGdkzBkk1q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4fae262a-0447-4e20-9f61-c9a423c30206"
      },
      "source": [
        "##########################################################################################333333\n",
        "#spelling issues\n",
        "\"\"\"\n",
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker() \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom spellchecker import SpellChecker\\nspell = SpellChecker() \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDaU6Ny9QGSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWUXnEzXkzUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d3f02caa-23fb-465d-fe01-49847ea4fd14"
      },
      "source": [
        "#correct \n",
        "#We're going to see them more like this, as part of the productive, organic framework of which they are inevitably a part, symbiotically connected.\n",
        "\n",
        "#worng\n",
        "#t=\"\"\"\n",
        "#We're going to see them more likee this, as part of the productive, organica framework of which thei are inevitably a part, symbiotically connected.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def take_data_to_shower(text):\n",
        "  alphanumeric = \"\"\n",
        "  for char in text:\n",
        "    if char ==\" \":\n",
        "      alphanumeric += \" \"\n",
        "      continue\n",
        "    if char.isalnum():\n",
        "      alphanumeric += char \n",
        "  return alphanumeric\n",
        "\n",
        "def spellingIssues(t):\n",
        "  text = take_data_to_shower(t)\n",
        "  wrong =[]\n",
        "  for w in (text.lower()).split():\n",
        "    flag = str(w)in words.words()\n",
        "    if flag == False:\n",
        "      wrong.append(w)\n",
        "  return len(wrong)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef take_data_to_shower(text):\\n  alphanumeric = \"\"\\n  for char in text:\\n    if char ==\" \":\\n      alphanumeric += \" \"\\n      continue\\n    if char.isalnum():\\n      alphanumeric += char \\n  return alphanumeric\\n\\ndef spellingIssues(t):\\n  text = take_data_to_shower(t)\\n  wrong =[]\\n  for w in (text.lower()).split():\\n    flag = str(w)in words.words()\\n    if flag == False:\\n      wrong.append(w)\\n  return len(wrong)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a7WngHGpxN1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0747b2f1-3217-4a0b-d05f-4fc656b358bb"
      },
      "source": [
        "\"\"\"\n",
        "print(\"spelling issues count = {}\".format(spellingIssues(t)))\n",
        "print(\"Worng words:\")\n",
        "print(str(wrong)[1:-1])\n",
        "print(\"correctness of worng words:\")\n",
        "for x in wrong:\n",
        "  print(\"{} ==> {}\".format(x,spell.correction(x)))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"spelling issues count = {}\".format(spellingIssues(t)))\\nprint(\"Worng words:\")\\nprint(str(wrong)[1:-1])\\nprint(\"correctness of worng words:\")\\nfor x in wrong:\\n  print(\"{} ==> {}\".format(x,spell.correction(x)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuxTXOHHzIUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################\n",
        "#Grammer issues\n",
        "tool = language_tool_python.LanguageTool('en-US')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fzh7sSWanQ95",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e9c1a2eb-5ddd-496f-bdbf-977dc7a6a863"
      },
      "source": [
        "#text = 'A sentence with a error in the Hitchhiker’s Guide tot he Galaxy'\n",
        "\"\"\"\n",
        "def grammarIsues(t):\n",
        "  matches = tool.check(t)\n",
        "  return (len(matches))\n",
        "print(\"Grammer issues count : {}\".format(len(matches)))\n",
        "print(\"wrong is : \")\n",
        "for i in matches:\n",
        "  print(\"{}\\n\".format(i))\n",
        "print(\"correct sentence is : {}\".format(tool.correct(text)))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef grammarIsues(t):\\n  matches = tool.check(t)\\n  return (len(matches))\\nprint(\"Grammer issues count : {}\".format(len(matches)))\\nprint(\"wrong is : \")\\nfor i in matches:\\n  print(\"{}\\n\".format(i))\\nprint(\"correct sentence is : {}\".format(tool.correct(text)))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGzc0kzlzoGV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7SKr6IKQKZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHHYZok5QNE_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3754d396-dac3-48b6-803d-a25e15f392a6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "float"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQiZ8rf3SKQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}